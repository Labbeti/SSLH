{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Supervised\n",
    "\n",
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "from mlu.datasets.utils import generate_indexes\n",
    "from mlu.metrics.categorical import CategoricalAccuracy\n",
    "from mlu.metrics.incremental import IncrementalMean\n",
    "from mlu.nn import CrossEntropyWithVectors, OneHot\n",
    "from mlu.utils.misc import reset_seed, get_datetime, get_lr, get_nb_parameters\n",
    "from mlu.utils.printers import LinePrinter\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor, RandomAffine, RandomHorizontalFlip, Compose"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.seed = 1234\n",
    "\n",
    "# Hyperparameters\n",
    "args.nb_epochs = 200\n",
    "args.bsize = 128\n",
    "args.nb_labels = 50000\n",
    "args.lr = 0.1\n",
    "\n",
    "# SGD parameters\n",
    "args.weight_decay = 0.0005\n",
    "args.momentum = 0.9  # called \"beta\" in paper\n",
    "args.nesterov = False\n",
    "\n",
    "# Scheduler parameters\n",
    "lr_decay_gamma = 0.2\n",
    "lr_decay_milestones = [60, 120, 160]\n",
    "\n",
    "optimizer_name = \"SGD\"  # or SGD\n",
    "\n",
    "reset_seed(args.seed)\n",
    "\n",
    "dataset_root = osp.join(\"..\", \"datasets\")\n",
    "tensorboard_root = osp.join(\"..\", \"results\", \"tensorboard\")\n",
    "device = torch.device(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "\t\"\"\"3x3 convolution with padding\"\"\"\n",
    "\treturn nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "\t\t\t\t\t padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "\t\"\"\"1x1 convolution\"\"\"\n",
    "\treturn nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\texpansion = 1\n",
    "\n",
    "\tdef __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "\t\t\t\t base_width=64, dilation=1, norm_layer=None):\n",
    "\t\tsuper(BasicBlock, self).__init__()\n",
    "\n",
    "\t\t# Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "\t\tself.conv1 = conv3x3(inplanes, planes, stride)\n",
    "\t\tself.bn1 = norm_layer(planes)\n",
    "\t\tself.relu = nn.ReLU(inplace=True)\n",
    "\t\tself.conv2 = conv3x3(planes, planes)\n",
    "\t\tself.bn2 = norm_layer(planes)\n",
    "\t\tself.downsample = downsample\n",
    "\t\tself.stride = stride\n",
    "\n",
    "\t\tself.expansion = 2\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tidentity = x\n",
    "\n",
    "\t\tout = self.conv1(x)\n",
    "\t\tout = self.bn1(out)\n",
    "\t\tout = self.relu(out)\n",
    "\n",
    "\t\tout = self.conv2(out)\n",
    "\t\tout = self.bn2(out)\n",
    "\n",
    "\t\tif self.downsample is not None:\n",
    "\t\t\tidentity = self.downsample(x)\n",
    "\n",
    "\t\tout += identity\n",
    "\t\tout = self.relu(out)\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class ResNet(Module):\n",
    "\tdef __init__(self, layers, width: int = 2, num_classes=10, zero_init_residual=False,\n",
    "\t\t\t\t groups=1, width_per_group=16, replace_stride_with_dilation=None,\n",
    "\t\t\t\t norm_layer=None):\n",
    "\t\tModule.__init__(self)\n",
    "\n",
    "\t\tif norm_layer is None:\n",
    "\t\t\tnorm_layer = nn.BatchNorm2d\n",
    "\t\tself._norm_layer = norm_layer\n",
    "\n",
    "\t\tblock = BasicBlock\n",
    "\t\tself.inplanes = 16*width\n",
    "\t\tself.dilation = 1\n",
    "\t\tif replace_stride_with_dilation is None:\n",
    "\t\t\t# each element in the tuple indicates if we should replace\n",
    "\t\t\t# the 2x2 stride with a dilated convolution instead\n",
    "\t\t\treplace_stride_with_dilation = [False, False, False]\n",
    "\t\tif len(replace_stride_with_dilation) != 3:\n",
    "\t\t\traise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "\t\t\t\t\t\t\t \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "\t\tself.groups = groups\n",
    "\t\tself.base_width = width_per_group\n",
    "\t\tself.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\t\tself.bn1 = norm_layer(self.inplanes)\n",
    "\t\tself.relu = nn.ReLU(inplace=True)\n",
    "\t\tself.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\t\tself.layer1 = self._make_layer(block, 16*width, layers[0])\n",
    "\t\tself.layer2 = self._make_layer(block, 32*width, layers[1], stride=2,\n",
    "\t\t\t\t\t\t\t\t\t   dilate=replace_stride_with_dilation[0])\n",
    "\t\tself.layer3 = self._make_layer(block, 64*width, layers[2], stride=2,\n",
    "\t\t\t\t\t\t\t\t\t   dilate=replace_stride_with_dilation[1])\n",
    "\n",
    "\t\tself.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\t\tself.fc = nn.Linear(64 * width * block.expansion, num_classes)\n",
    "\n",
    "\t\tfor m in self.modules():\n",
    "\t\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "\t\t\telif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "\t\t\t\tnn.init.constant_(m.weight, 1)\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\n",
    "\t\t# Zero-initialize the last BN in each residual branch,\n",
    "\t\t# so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "\t\t# This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "\t\tif zero_init_residual:\n",
    "\t\t\tfor m in self.modules():\n",
    "\t\t\t\tif isinstance(m, BasicBlock):\n",
    "\t\t\t\t\tnn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "\tdef _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "\t\tnorm_layer = self._norm_layer\n",
    "\t\tdownsample = None\n",
    "\t\tprevious_dilation = self.dilation\n",
    "\t\tif dilate:\n",
    "\t\t\tself.dilation *= stride\n",
    "\t\t\tstride = 1\n",
    "\t\tif stride != 1 or self.inplanes != planes * block.expansion:\n",
    "\t\t\tdownsample = nn.Sequential(\n",
    "\t\t\t\tconv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "\t\t\t\tnorm_layer(planes * block.expansion),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tlayers = []\n",
    "\t\tlayers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "\t\t\t\t\t\t\tself.base_width, previous_dilation, norm_layer))\n",
    "\t\tself.inplanes = planes * block.expansion\n",
    "\t\tfor _ in range(1, blocks):\n",
    "\t\t\tlayers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "\t\t\t\t\t\t\t\tbase_width=self.base_width, dilation=self.dilation,\n",
    "\t\t\t\t\t\t\t\tnorm_layer=norm_layer))\n",
    "\n",
    "\t\treturn nn.Sequential(*layers)\n",
    "\n",
    "\tdef _forward_impl(self, x):\n",
    "\t\t# See note [TorchScript super()]\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.bn1(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\tx = self.maxpool(x)\n",
    "\n",
    "\t\tx = self.layer1(x)\n",
    "\t\tx = self.layer2(x)\n",
    "\t\tx = self.layer3(x)\n",
    "\n",
    "\t\tx = self.avgpool(x)\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\t\tx = self.fc(x)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self._forward_impl(x)\n",
    "\n",
    "\n",
    "class WideResNet28(ResNet):\n",
    "\tdef __init__(self, num_classes: int, width: int = 2):\n",
    "\t\tsuper().__init__(layers=[4, 4, 4], width=width, num_classes=num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build models, optimizer, metrics, and utilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb model parameters :  1472554\n"
     ]
    }
   ],
   "source": [
    "# Build WideResNet-28-2 model\n",
    "model = WideResNet28(num_classes=10, width=2).to(device)\n",
    "activation = lambda x, dim: x.softmax(dim).clamp(2e-20)  # lambda x, dim: x\n",
    "criterion = CrossEntropyWithVectors()\n",
    "\n",
    "if optimizer_name.upper() == \"SGD\":\n",
    "\toptim = SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=args.momentum, nesterov=args.nesterov)\n",
    "else:\n",
    "\toptim = Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = MultiStepLR(optim, milestones=lr_decay_milestones, gamma=lr_decay_gamma)\n",
    "\n",
    "# Build metrics for labeled, unlabeled and validation predictions.\n",
    "metrics_train = {\"train/acc\": CategoricalAccuracy(vector_target=True)}\n",
    "metrics_val = {\"val/acc\": CategoricalAccuracy(vector_target=True)}\n",
    "\n",
    "# Tensorboard writer and the Recorder wrapper for tracking max, std & min of the values stored.\n",
    "writer = SummaryWriter(osp.join(tensorboard_root, \"CIFAR10_%s_WideResNet28_Supervised_Notebook\" % get_datetime()))\n",
    "\n",
    "# Class for managing how the values are print in terminal\n",
    "printer = LinePrinter()\n",
    "\n",
    "print(\"Nb model parameters : \", get_nb_parameters(model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Builds datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "transform_train = Compose([\n",
    "\tRandomAffine(0, translate=(1/16, 1/16)),\n",
    "\tRandomHorizontalFlip(),\n",
    "\tToTensor(),\n",
    "])\n",
    "transform_val = Compose([\n",
    "    ToTensor(),\n",
    "])\n",
    "target_transform = OneHot(nb_classes=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Use 50000 labels. (proportion of the dataset = 1.0/1.0)\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_train = CIFAR10(dataset_root, train=True, download=True, transform=transform_train, target_transform=target_transform)\n",
    "\n",
    "supervised_ratio = args.nb_labels / len(dataset_train)\n",
    "if args.nb_labels < len(dataset_train):\n",
    "\tindexes_s = generate_indexes(\n",
    "\t\tdataset_train,\n",
    "\t\tnb_classes=10,\n",
    "\t\tratios=[supervised_ratio],\n",
    "\t\ttarget_one_hot=True,\n",
    "\t)[0]\n",
    "\tdataset_train = Subset(dataset_train, indexes_s)\n",
    "\n",
    "print(\"Use {} labels. (proportion of the dataset = {}/1.0)\".format(args.nb_labels, supervised_ratio))\n",
    "\n",
    "# Create validation dataset\n",
    "dataset_val = CIFAR10(dataset_root, train=False, download=True, transform=transform_val, target_transform=target_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build loaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loader_train = DataLoader(dataset_train, batch_size=args.bsize, shuffle=True, num_workers=4, drop_last=False)\n",
    "loader_val = DataLoader(dataset_val, batch_size=args.bsize, shuffle=False, drop_last=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "\tmodel.train()\n",
    "\n",
    "\tmetric_names = [\"train/loss\"] + list(metrics_train.keys())\n",
    "\tcontinue_metrics = {name: IncrementalMean() for name in metric_names}\n",
    "\n",
    "\tfor i, (batch, labels) in enumerate(loader_train):\n",
    "\t\tbatch = batch.to(device).float()\n",
    "\t\tlabels = labels.to(device).long()\n",
    "\n",
    "\t\toptim.zero_grad()\n",
    "\n",
    "\t\t# Compute prediction\n",
    "\t\tlogits = model(batch)\n",
    "\t\tpred = activation(logits, dim=1)\n",
    "\n",
    "\t\t# Update model\n",
    "\t\tloss = criterion(pred, labels)\n",
    "\t\tloss.backward()\n",
    "\t\toptim.step()\n",
    "\n",
    "\t\t# Compute metrics\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tcontinue_metrics[\"train/loss\"].add(loss.item())\n",
    "\n",
    "\t\t\tfor name, metric in metrics_train.items():\n",
    "\t\t\t\tscore = metric(pred, labels)\n",
    "\t\t\t\tcontinue_metrics[name].add(score.item())\n",
    "\n",
    "\t\t\tcurrent_values = {name: continue_metric.get_current() for name, continue_metric in continue_metrics.items()}\n",
    "\t\t\tcurrent_values[\"train/lr\"] = get_lr(optim)\n",
    "\t\t\tprinter.print_current_values(current_values, i, len(loader_train), epoch)\n",
    "\n",
    "\t# Save metrics in tensorboard\n",
    "\tfor name, continue_metric in continue_metrics.items():\n",
    "\t\twriter.add_scalar(name, continue_metric.get_current(), epoch)\n",
    "\twriter.add_scalar(\"train/lr\", get_lr(optim), epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def val(epoch: int):\n",
    "\tmodel.eval()\n",
    "\n",
    "\tmetric_names = list(metrics_val.keys())\n",
    "\tcontinue_metrics = {name: IncrementalMean() for name in metric_names}\n",
    "\n",
    "\tfor i, (x, y) in enumerate(loader_val):\n",
    "\t\tx = x.to(device).float()\n",
    "\t\ty = y.to(device).long()\n",
    "\n",
    "\t\t# Compute prediction\n",
    "\t\tlogits = model(x)\n",
    "\t\tpred = torch.softmax(logits, dim=1)\n",
    "\n",
    "\t\tfor name, metric in metrics_val.items():\n",
    "\t\t\tscore = metric(pred, y)\n",
    "\t\t\tcontinue_metrics[name].add(score.item())\n",
    "\n",
    "\t\tcurrent_values = {name: continue_metric.get_current() for name, continue_metric in continue_metrics.items()}\n",
    "\t\tprinter.print_current_values(current_values, i, len(loader_val), epoch)\n",
    "\n",
    "\t# Save metrics in tensorboard\n",
    "\tfor name, continue_metric in continue_metrics.items():\n",
    "\t\twriter.add_scalar(name, continue_metric.get_current(), epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, epoch   1, 100%, acc: 3.6314e-01, loss: 1.7386e+00, lr: 1.0000e-01, took (s): 6.95\r\n",
      "val  , epoch   1, 100%, acc: 5.0307e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch   2, 100%, acc: 5.5263e-01, loss: 1.2334e+00, lr: 1.0000e-01, took (s): 7.04\r\n",
      "val  , epoch   2, 100%, acc: 5.2146e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch   3, 100%, acc: 6.3858e-01, loss: 1.0188e+00, lr: 1.0000e-01, took (s): 7.10\r\n",
      "val  , epoch   3, 100%, acc: 6.6525e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch   4, 100%, acc: 6.8548e-01, loss: 8.9032e-01, lr: 1.0000e-01, took (s): 6.92\r\n",
      "val  , epoch   4, 100%, acc: 6.5803e-01, took (s): 2.08\r\n",
      "\n",
      "train, epoch   5, 100%, acc: 7.1624e-01, loss: 8.0996e-01, lr: 1.0000e-01, took (s): 7.76\r\n",
      "val  , epoch   5, 100%, acc: 6.0117e-01, took (s): 2.05\r\n",
      "\n",
      "train, epoch   6, 100%, acc: 7.3955e-01, loss: 7.4932e-01, lr: 1.0000e-01, took (s): 6.94\r\n",
      "val  , epoch   6, 100%, acc: 7.1737e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch   7, 100%, acc: 7.5805e-01, loss: 6.9850e-01, lr: 1.0000e-01, took (s): 6.92\r\n",
      "val  , epoch   7, 100%, acc: 7.3190e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch   8, 100%, acc: 7.7633e-01, loss: 6.5314e-01, lr: 1.0000e-01, took (s): 7.00\r\n",
      "val  , epoch   8, 100%, acc: 7.2429e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch   9, 100%, acc: 7.8227e-01, loss: 6.2758e-01, lr: 1.0000e-01, took (s): 6.92\r\n",
      "val  , epoch   9, 100%, acc: 7.4911e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  10, 100%, acc: 7.9238e-01, loss: 6.0065e-01, lr: 1.0000e-01, took (s): 6.99\r\n",
      "val  , epoch  10, 100%, acc: 7.7017e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch  11, 100%, acc: 7.9759e-01, loss: 5.8888e-01, lr: 1.0000e-01, took (s): 7.15\r\n",
      "val  , epoch  11, 100%, acc: 7.4219e-01, took (s): 2.06\r\n",
      "\n",
      "train, epoch  12, 100%, acc: 8.0518e-01, loss: 5.7052e-01, lr: 1.0000e-01, took (s): 7.07\r\n",
      "val  , epoch  12, 100%, acc: 7.3586e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch  13, 100%, acc: 8.0579e-01, loss: 5.6290e-01, lr: 1.0000e-01, took (s): 6.96\r\n",
      "val  , epoch  13, 100%, acc: 7.1905e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  14, 100%, acc: 8.1081e-01, loss: 5.4562e-01, lr: 1.0000e-01, took (s): 7.00\r\n",
      "val  , epoch  14, 100%, acc: 7.6681e-01, took (s): 1.96\r\n",
      "\n",
      "train, epoch  15, 100%, acc: 8.1367e-01, loss: 5.4224e-01, lr: 1.0000e-01, took (s): 7.47\r\n",
      "val  , epoch  15, 100%, acc: 7.7957e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch  16, 100%, acc: 8.1739e-01, loss: 5.3165e-01, lr: 1.0000e-01, took (s): 7.11\r\n",
      "val  , epoch  16, 100%, acc: 7.6473e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch  17, 100%, acc: 8.2044e-01, loss: 5.3038e-01, lr: 1.0000e-01, took (s): 7.11\r\n",
      "val  , epoch  17, 100%, acc: 7.5742e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch  18, 100%, acc: 8.2316e-01, loss: 5.1792e-01, lr: 1.0000e-01, took (s): 7.26\r\n",
      "val  , epoch  18, 100%, acc: 8.0202e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  19, 100%, acc: 8.2260e-01, loss: 5.1493e-01, lr: 1.0000e-01, took (s): 7.05\r\n",
      "val  , epoch  19, 100%, acc: 7.0669e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  20, 100%, acc: 8.2745e-01, loss: 5.0522e-01, lr: 1.0000e-01, took (s): 7.13\r\n",
      "val  , epoch  20, 100%, acc: 7.7324e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  21, 100%, acc: 8.2696e-01, loss: 5.0729e-01, lr: 1.0000e-01, took (s): 7.03\r\n",
      "val  , epoch  21, 100%, acc: 7.7858e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  22, 100%, acc: 8.2755e-01, loss: 5.0225e-01, lr: 1.0000e-01, took (s): 7.19\r\n",
      "val  , epoch  22, 100%, acc: 8.0113e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch  23, 100%, acc: 8.3257e-01, loss: 4.8755e-01, lr: 1.0000e-01, took (s): 7.40\r\n",
      "val  , epoch  23, 100%, acc: 7.5781e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  24, 100%, acc: 8.3041e-01, loss: 4.9463e-01, lr: 1.0000e-01, took (s): 7.01\r\n",
      "val  , epoch  24, 100%, acc: 8.1665e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  25, 100%, acc: 8.2916e-01, loss: 4.9643e-01, lr: 1.0000e-01, took (s): 6.94\r\n",
      "val  , epoch  25, 100%, acc: 8.0963e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  26, 100%, acc: 8.3320e-01, loss: 4.8598e-01, lr: 1.0000e-01, took (s): 7.06\r\n",
      "val  , epoch  26, 100%, acc: 7.4387e-01, took (s): 2.05\r\n",
      "\n",
      "train, epoch  27, 100%, acc: 8.3424e-01, loss: 4.8277e-01, lr: 1.0000e-01, took (s): 7.30\r\n",
      "val  , epoch  27, 100%, acc: 7.9697e-01, took (s): 2.08\r\n",
      "\n",
      "train, epoch  28, 100%, acc: 8.3485e-01, loss: 4.8409e-01, lr: 1.0000e-01, took (s): 7.26\r\n",
      "val  , epoch  28, 100%, acc: 7.4446e-01, took (s): 2.06\r\n",
      "\n",
      "train, epoch  29, 100%, acc: 8.3799e-01, loss: 4.7594e-01, lr: 1.0000e-01, took (s): 7.11\r\n",
      "val  , epoch  29, 100%, acc: 7.5969e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  30, 100%, acc: 8.3685e-01, loss: 4.7890e-01, lr: 1.0000e-01, took (s): 7.52\r\n",
      "val  , epoch  30, 100%, acc: 7.7146e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  31, 100%, acc: 8.3743e-01, loss: 4.7311e-01, lr: 1.0000e-01, took (s): 7.14\r\n",
      "val  , epoch  31, 100%, acc: 7.9203e-01, took (s): 2.06\r\n",
      "\n",
      "train, epoch  32, 100%, acc: 8.3732e-01, loss: 4.7783e-01, lr: 1.0000e-01, took (s): 7.12\r\n",
      "val  , epoch  32, 100%, acc: 7.5930e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  33, 100%, acc: 8.4170e-01, loss: 4.6216e-01, lr: 1.0000e-01, took (s): 7.05\r\n",
      "val  , epoch  33, 100%, acc: 7.5979e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  34, 100%, acc: 8.4281e-01, loss: 4.6099e-01, lr: 1.0000e-01, took (s): 6.99\r\n",
      "val  , epoch  34, 100%, acc: 8.0795e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  35, 100%, acc: 8.3948e-01, loss: 4.6554e-01, lr: 1.0000e-01, took (s): 7.02\r\n",
      "val  , epoch  35, 100%, acc: 7.9055e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  36, 100%, acc: 8.4208e-01, loss: 4.6101e-01, lr: 1.0000e-01, took (s): 6.94\r\n",
      "val  , epoch  36, 100%, acc: 8.0014e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  37, 100%, acc: 8.4203e-01, loss: 4.6185e-01, lr: 1.0000e-01, took (s): 6.95\r\n",
      "val  , epoch  37, 100%, acc: 8.0004e-01, took (s): 1.96\r\n",
      "\n",
      "train, epoch  38, 100%, acc: 8.4149e-01, loss: 4.6422e-01, lr: 1.0000e-01, took (s): 7.08\r\n",
      "val  , epoch  38, 100%, acc: 7.3527e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  39, 100%, acc: 8.4138e-01, loss: 4.6648e-01, lr: 1.0000e-01, took (s): 7.01\r\n",
      "val  , epoch  39, 100%, acc: 7.5534e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  40, 100%, acc: 8.4405e-01, loss: 4.5605e-01, lr: 1.0000e-01, took (s): 7.06\r\n",
      "val  , epoch  40, 100%, acc: 8.0924e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  41, 100%, acc: 8.4722e-01, loss: 4.5223e-01, lr: 1.0000e-01, took (s): 7.00\r\n",
      "val  , epoch  41, 100%, acc: 7.7314e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  42, 100%, acc: 8.4307e-01, loss: 4.5581e-01, lr: 1.0000e-01, took (s): 6.96\r\n",
      "val  , epoch  42, 100%, acc: 7.7344e-01, took (s): 1.98\r\n",
      "\n",
      "train, epoch  43, 100%, acc: 8.4418e-01, loss: 4.5269e-01, lr: 1.0000e-01, took (s): 6.98\r\n",
      "val  , epoch  43, 100%, acc: 8.0657e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch  44, 100%, acc: 8.4305e-01, loss: 4.5267e-01, lr: 1.0000e-01, took (s): 7.00\r\n",
      "val  , epoch  44, 100%, acc: 8.0439e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  45, 100%, acc: 8.4429e-01, loss: 4.5238e-01, lr: 1.0000e-01, took (s): 7.11\r\n",
      "val  , epoch  45, 100%, acc: 8.0706e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  46, 100%, acc: 8.4369e-01, loss: 4.5590e-01, lr: 1.0000e-01, took (s): 7.14\r\n",
      "val  , epoch  46, 100%, acc: 8.0419e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch  47, 100%, acc: 8.4416e-01, loss: 4.5227e-01, lr: 1.0000e-01, took (s): 6.99\r\n",
      "val  , epoch  47, 100%, acc: 7.8659e-01, took (s): 1.98\r\n",
      "\n",
      "train, epoch  48, 100%, acc: 8.4509e-01, loss: 4.4928e-01, lr: 1.0000e-01, took (s): 7.05\r\n",
      "val  , epoch  48, 100%, acc: 7.6108e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch  49, 100%, acc: 8.4500e-01, loss: 4.5083e-01, lr: 1.0000e-01, took (s): 6.95\r\n",
      "val  , epoch  49, 100%, acc: 8.1705e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  50, 100%, acc: 8.4524e-01, loss: 4.5362e-01, lr: 1.0000e-01, took (s): 6.91\r\n",
      "val  , epoch  50, 100%, acc: 7.8936e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  51, 100%, acc: 8.4781e-01, loss: 4.4464e-01, lr: 1.0000e-01, took (s): 7.24\r\n",
      "val  , epoch  51, 100%, acc: 7.8076e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch  52, 100%, acc: 8.4808e-01, loss: 4.4469e-01, lr: 1.0000e-01, took (s): 7.09\r\n",
      "val  , epoch  52, 100%, acc: 8.0568e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  53, 100%, acc: 8.4711e-01, loss: 4.4918e-01, lr: 1.0000e-01, took (s): 7.03\r\n",
      "val  , epoch  53, 100%, acc: 8.0736e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch  54, 100%, acc: 8.4763e-01, loss: 4.4427e-01, lr: 1.0000e-01, took (s): 7.03\r\n",
      "val  , epoch  54, 100%, acc: 7.8323e-01, took (s): 1.97\r\n",
      "\n",
      "train, epoch  55, 100%, acc: 8.4666e-01, loss: 4.4946e-01, lr: 1.0000e-01, took (s): 7.06\r\n",
      "val  , epoch  55, 100%, acc: 8.1042e-01, took (s): 1.96\r\n",
      "\n",
      "train, epoch  56, 100%, acc: 8.4932e-01, loss: 4.4271e-01, lr: 1.0000e-01, took (s): 7.07\r\n",
      "val  , epoch  56, 100%, acc: 8.2991e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  57, 100%, acc: 8.4736e-01, loss: 4.4578e-01, lr: 1.0000e-01, took (s): 6.98\r\n",
      "val  , epoch  57, 100%, acc: 7.6187e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  58, 100%, acc: 8.4895e-01, loss: 4.4138e-01, lr: 1.0000e-01, took (s): 7.00\r\n",
      "val  , epoch  58, 100%, acc: 7.4901e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  59, 100%, acc: 8.4847e-01, loss: 4.4483e-01, lr: 1.0000e-01, took (s): 6.97\r\n",
      "val  , epoch  59, 100%, acc: 7.7937e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch  60, 100%, acc: 8.4814e-01, loss: 4.4272e-01, lr: 1.0000e-01, took (s): 6.99\r\n",
      "val  , epoch  60, 100%, acc: 8.1112e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch  61, 100%, acc: 9.0901e-01, loss: 2.6801e-01, lr: 2.0000e-02, took (s): 6.99\r\n",
      "val  , epoch  61, 100%, acc: 8.8677e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  62, 100%, acc: 9.2366e-01, loss: 2.2298e-01, lr: 2.0000e-02, took (s): 7.04\r\n",
      "val  , epoch  62, 100%, acc: 8.9142e-01, took (s): 1.98\r\n",
      "\n",
      "train, epoch  63, 100%, acc: 9.3157e-01, loss: 1.9868e-01, lr: 2.0000e-02, took (s): 7.04\r\n",
      "val  , epoch  63, 100%, acc: 8.9527e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  64, 100%, acc: 9.3332e-01, loss: 1.9128e-01, lr: 2.0000e-02, took (s): 7.08\r\n",
      "val  , epoch  64, 100%, acc: 8.8766e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  65, 100%, acc: 9.3698e-01, loss: 1.8452e-01, lr: 2.0000e-02, took (s): 7.11\r\n",
      "val  , epoch  65, 100%, acc: 8.9211e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  66, 100%, acc: 9.3941e-01, loss: 1.7611e-01, lr: 2.0000e-02, took (s): 7.30\r\n",
      "val  , epoch  66, 100%, acc: 8.8756e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  67, 100%, acc: 9.4011e-01, loss: 1.7482e-01, lr: 2.0000e-02, took (s): 7.12\r\n",
      "val  , epoch  67, 100%, acc: 8.9112e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  68, 100%, acc: 9.4185e-01, loss: 1.7115e-01, lr: 2.0000e-02, took (s): 7.14\r\n",
      "val  , epoch  68, 100%, acc: 8.9122e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  69, 100%, acc: 9.3938e-01, loss: 1.7484e-01, lr: 2.0000e-02, took (s): 7.45\r\n",
      "val  , epoch  69, 100%, acc: 8.9320e-01, took (s): 2.06\r\n",
      "\n",
      "train, epoch  70, 100%, acc: 9.4181e-01, loss: 1.6977e-01, lr: 2.0000e-02, took (s): 7.08\r\n",
      "val  , epoch  70, 100%, acc: 8.8558e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  71, 100%, acc: 9.3960e-01, loss: 1.7214e-01, lr: 2.0000e-02, took (s): 7.00\r\n",
      "val  , epoch  71, 100%, acc: 8.7490e-01, took (s): 1.98\r\n",
      "\n",
      "train, epoch  72, 100%, acc: 9.3987e-01, loss: 1.7090e-01, lr: 2.0000e-02, took (s): 7.12\r\n",
      "val  , epoch  72, 100%, acc: 8.8568e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  73, 100%, acc: 9.3955e-01, loss: 1.7759e-01, lr: 2.0000e-02, took (s): 7.06\r\n",
      "val  , epoch  73, 100%, acc: 8.7718e-01, took (s): 2.06\r\n",
      "\n",
      "train, epoch  74, 100%, acc: 9.3749e-01, loss: 1.7787e-01, lr: 2.0000e-02, took (s): 7.22\r\n",
      "val  , epoch  74, 100%, acc: 8.6798e-01, took (s): 2.10\r\n",
      "\n",
      "train, epoch  75, 100%, acc: 9.3672e-01, loss: 1.8048e-01, lr: 2.0000e-02, took (s): 7.10\r\n",
      "val  , epoch  75, 100%, acc: 8.6837e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  76, 100%, acc: 9.3747e-01, loss: 1.7862e-01, lr: 2.0000e-02, took (s): 7.29\r\n",
      "val  , epoch  76, 100%, acc: 8.7609e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  77, 100%, acc: 9.3646e-01, loss: 1.7894e-01, lr: 2.0000e-02, took (s): 7.27\r\n",
      "val  , epoch  77, 100%, acc: 8.7065e-01, took (s): 2.05\r\n",
      "\n",
      "train, epoch  78, 100%, acc: 9.3910e-01, loss: 1.7477e-01, lr: 2.0000e-02, took (s): 7.12\r\n",
      "val  , epoch  78, 100%, acc: 8.6185e-01, took (s): 2.08\r\n",
      "\n",
      "train, epoch  79, 100%, acc: 9.3768e-01, loss: 1.7663e-01, lr: 2.0000e-02, took (s): 7.23\r\n",
      "val  , epoch  79, 100%, acc: 8.8232e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  80, 100%, acc: 9.3794e-01, loss: 1.7444e-01, lr: 2.0000e-02, took (s): 7.21\r\n",
      "val  , epoch  80, 100%, acc: 8.8083e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  81, 100%, acc: 9.3961e-01, loss: 1.7427e-01, lr: 2.0000e-02, took (s): 7.02\r\n",
      "val  , epoch  81, 100%, acc: 8.7233e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch  82, 100%, acc: 9.3775e-01, loss: 1.7733e-01, lr: 2.0000e-02, took (s): 7.05\r\n",
      "val  , epoch  82, 100%, acc: 8.7520e-01, took (s): 2.05\r\n",
      "\n",
      "train, epoch  83, 100%, acc: 9.3850e-01, loss: 1.7353e-01, lr: 2.0000e-02, took (s): 7.13\r\n",
      "val  , epoch  83, 100%, acc: 8.4800e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  84, 100%, acc: 9.4095e-01, loss: 1.7143e-01, lr: 2.0000e-02, took (s): 7.09\r\n",
      "val  , epoch  84, 100%, acc: 8.6462e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  85, 100%, acc: 9.3835e-01, loss: 1.7545e-01, lr: 2.0000e-02, took (s): 7.05\r\n",
      "val  , epoch  85, 100%, acc: 8.7925e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  86, 100%, acc: 9.4043e-01, loss: 1.7266e-01, lr: 2.0000e-02, took (s): 7.13\r\n",
      "val  , epoch  86, 100%, acc: 8.6630e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  87, 100%, acc: 9.3869e-01, loss: 1.7276e-01, lr: 2.0000e-02, took (s): 7.19\r\n",
      "val  , epoch  87, 100%, acc: 8.7787e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  88, 100%, acc: 9.4263e-01, loss: 1.6708e-01, lr: 2.0000e-02, took (s): 7.10\r\n",
      "val  , epoch  88, 100%, acc: 8.8439e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  89, 100%, acc: 9.4030e-01, loss: 1.7289e-01, lr: 2.0000e-02, took (s): 7.02\r\n",
      "val  , epoch  89, 100%, acc: 8.7015e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch  90, 100%, acc: 9.4042e-01, loss: 1.6993e-01, lr: 2.0000e-02, took (s): 7.35\r\n",
      "val  , epoch  90, 100%, acc: 8.7154e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  91, 100%, acc: 9.4261e-01, loss: 1.6495e-01, lr: 2.0000e-02, took (s): 7.03\r\n",
      "val  , epoch  91, 100%, acc: 8.8014e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch  92, 100%, acc: 9.4013e-01, loss: 1.6966e-01, lr: 2.0000e-02, took (s): 7.15\r\n",
      "val  , epoch  92, 100%, acc: 8.6897e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch  93, 100%, acc: 9.4142e-01, loss: 1.6813e-01, lr: 2.0000e-02, took (s): 7.10\r\n",
      "val  , epoch  93, 100%, acc: 8.7866e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  94, 100%, acc: 9.4308e-01, loss: 1.6159e-01, lr: 2.0000e-02, took (s): 7.27\r\n",
      "val  , epoch  94, 100%, acc: 8.8202e-01, took (s): 2.10\r\n",
      "\n",
      "train, epoch  95, 100%, acc: 9.4162e-01, loss: 1.6729e-01, lr: 2.0000e-02, took (s): 7.09\r\n",
      "val  , epoch  95, 100%, acc: 8.7490e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch  96, 100%, acc: 9.4052e-01, loss: 1.6798e-01, lr: 2.0000e-02, took (s): 7.08\r\n",
      "val  , epoch  96, 100%, acc: 8.7985e-01, took (s): 2.11\r\n",
      "\n",
      "train, epoch  97, 100%, acc: 9.4309e-01, loss: 1.6432e-01, lr: 2.0000e-02, took (s): 7.15\r\n",
      "val  , epoch  97, 100%, acc: 8.5285e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch  98, 100%, acc: 9.4405e-01, loss: 1.6309e-01, lr: 2.0000e-02, took (s): 7.27\r\n",
      "val  , epoch  98, 100%, acc: 8.7085e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch  99, 100%, acc: 9.4546e-01, loss: 1.5844e-01, lr: 2.0000e-02, took (s): 7.14\r\n",
      "val  , epoch  99, 100%, acc: 8.7342e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 100, 100%, acc: 9.4315e-01, loss: 1.6091e-01, lr: 2.0000e-02, took (s): 7.40\r\n",
      "val  , epoch 100, 100%, acc: 8.7282e-01, took (s): 2.09\r\n",
      "\n",
      "train, epoch 101, 100%, acc: 9.4237e-01, loss: 1.6402e-01, lr: 2.0000e-02, took (s): 7.11\r\n",
      "val  , epoch 101, 100%, acc: 8.6966e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 102, 100%, acc: 9.4464e-01, loss: 1.5752e-01, lr: 2.0000e-02, took (s): 7.46\r\n",
      "val  , epoch 102, 100%, acc: 8.6778e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 103, 100%, acc: 9.4368e-01, loss: 1.6265e-01, lr: 2.0000e-02, took (s): 7.06\r\n",
      "val  , epoch 103, 100%, acc: 8.7955e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 104, 100%, acc: 9.4403e-01, loss: 1.6026e-01, lr: 2.0000e-02, took (s): 7.15\r\n",
      "val  , epoch 104, 100%, acc: 8.7836e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch 105, 100%, acc: 9.4528e-01, loss: 1.5837e-01, lr: 2.0000e-02, took (s): 7.08\r\n",
      "val  , epoch 105, 100%, acc: 8.7846e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch 106, 100%, acc: 9.4578e-01, loss: 1.5524e-01, lr: 2.0000e-02, took (s): 7.07\r\n",
      "val  , epoch 106, 100%, acc: 8.6986e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 107, 100%, acc: 9.4494e-01, loss: 1.5756e-01, lr: 2.0000e-02, took (s): 7.16\r\n",
      "val  , epoch 107, 100%, acc: 8.7727e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch 108, 100%, acc: 9.4589e-01, loss: 1.5612e-01, lr: 2.0000e-02, took (s): 7.08\r\n",
      "val  , epoch 108, 100%, acc: 8.7856e-01, took (s): 2.05\r\n",
      "\n",
      "train, epoch 109, 100%, acc: 9.4623e-01, loss: 1.5775e-01, lr: 2.0000e-02, took (s): 7.14\r\n",
      "val  , epoch 109, 100%, acc: 8.8341e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch 110, 100%, acc: 9.4584e-01, loss: 1.5559e-01, lr: 2.0000e-02, took (s): 7.13\r\n",
      "val  , epoch 110, 100%, acc: 8.6768e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 111, 100%, acc: 9.4782e-01, loss: 1.5210e-01, lr: 2.0000e-02, took (s): 7.05\r\n",
      "val  , epoch 111, 100%, acc: 8.7203e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 112, 100%, acc: 9.4641e-01, loss: 1.5499e-01, lr: 2.0000e-02, took (s): 7.08\r\n",
      "val  , epoch 112, 100%, acc: 8.6442e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 113, 100%, acc: 9.4548e-01, loss: 1.5666e-01, lr: 2.0000e-02, took (s): 7.18\r\n",
      "val  , epoch 113, 100%, acc: 8.6303e-01, took (s): 2.11\r\n",
      "\n",
      "train, epoch 114, 100%, acc: 9.4725e-01, loss: 1.5331e-01, lr: 2.0000e-02, took (s): 7.63\r\n",
      "val  , epoch 114, 100%, acc: 8.7886e-01, took (s): 2.08\r\n",
      "\n",
      "train, epoch 115, 100%, acc: 9.4840e-01, loss: 1.5102e-01, lr: 2.0000e-02, took (s): 7.52\r\n",
      "val  , epoch 115, 100%, acc: 8.8360e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 116, 100%, acc: 9.4688e-01, loss: 1.5425e-01, lr: 2.0000e-02, took (s): 7.47\r\n",
      "val  , epoch 116, 100%, acc: 8.7441e-01, took (s): 2.14\r\n",
      "\n",
      "train, epoch 117, 100%, acc: 9.4961e-01, loss: 1.4868e-01, lr: 2.0000e-02, took (s): 7.14\r\n",
      "val  , epoch 117, 100%, acc: 8.7460e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 118, 100%, acc: 9.5051e-01, loss: 1.4650e-01, lr: 2.0000e-02, took (s): 7.19\r\n",
      "val  , epoch 118, 100%, acc: 8.6847e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch 119, 100%, acc: 9.4536e-01, loss: 1.5591e-01, lr: 2.0000e-02, took (s): 7.14\r\n",
      "val  , epoch 119, 100%, acc: 8.7263e-01, took (s): 2.10\r\n",
      "\n",
      "train, epoch 120, 100%, acc: 9.4775e-01, loss: 1.5229e-01, lr: 2.0000e-02, took (s): 7.47\r\n",
      "val  , epoch 120, 100%, acc: 8.7460e-01, took (s): 2.10\r\n",
      "\n",
      "train, epoch 121, 100%, acc: 9.7671e-01, loss: 7.2624e-02, lr: 4.0000e-03, took (s): 7.54\r\n",
      "val  , epoch 121, 100%, acc: 9.0477e-01, took (s): 2.09\r\n",
      "\n",
      "train, epoch 122, 100%, acc: 9.8489e-01, loss: 4.9159e-02, lr: 4.0000e-03, took (s): 7.58\r\n",
      "val  , epoch 122, 100%, acc: 9.0714e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch 123, 100%, acc: 9.8833e-01, loss: 3.9494e-02, lr: 4.0000e-03, took (s): 7.51\r\n",
      "val  , epoch 123, 100%, acc: 9.0951e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch 124, 100%, acc: 9.9027e-01, loss: 3.4121e-02, lr: 4.0000e-03, took (s): 7.23\r\n",
      "val  , epoch 124, 100%, acc: 9.0981e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch 125, 100%, acc: 9.9134e-01, loss: 2.9582e-02, lr: 4.0000e-03, took (s): 7.08\r\n",
      "val  , epoch 125, 100%, acc: 9.1238e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 126, 100%, acc: 9.9234e-01, loss: 2.6162e-02, lr: 4.0000e-03, took (s): 7.07\r\n",
      "val  , epoch 126, 100%, acc: 9.1179e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 127, 100%, acc: 9.9299e-01, loss: 2.3628e-02, lr: 4.0000e-03, took (s): 7.11\r\n",
      "val  , epoch 127, 100%, acc: 9.1159e-01, took (s): 1.98\r\n",
      "\n",
      "train, epoch 128, 100%, acc: 9.9314e-01, loss: 2.2774e-02, lr: 4.0000e-03, took (s): 7.06\r\n",
      "val  , epoch 128, 100%, acc: 9.1080e-01, took (s): 2.06\r\n",
      "\n",
      "train, epoch 129, 100%, acc: 9.9355e-01, loss: 2.1903e-02, lr: 4.0000e-03, took (s): 7.08\r\n",
      "val  , epoch 129, 100%, acc: 9.1100e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 130, 100%, acc: 9.9415e-01, loss: 2.0668e-02, lr: 4.0000e-03, took (s): 7.07\r\n",
      "val  , epoch 130, 100%, acc: 9.1357e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 131, 100%, acc: 9.9405e-01, loss: 2.0083e-02, lr: 4.0000e-03, took (s): 7.21\r\n",
      "val  , epoch 131, 100%, acc: 9.1119e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 132, 100%, acc: 9.9532e-01, loss: 1.7558e-02, lr: 4.0000e-03, took (s): 7.32\r\n",
      "val  , epoch 132, 100%, acc: 9.1238e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 133, 100%, acc: 9.9499e-01, loss: 1.7514e-02, lr: 4.0000e-03, took (s): 7.20\r\n",
      "val  , epoch 133, 100%, acc: 9.1377e-01, took (s): 2.08\r\n",
      "\n",
      "train, epoch 134, 100%, acc: 9.9521e-01, loss: 1.7493e-02, lr: 4.0000e-03, took (s): 7.12\r\n",
      "val  , epoch 134, 100%, acc: 9.1179e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch 135, 100%, acc: 9.9598e-01, loss: 1.4839e-02, lr: 4.0000e-03, took (s): 7.09\r\n",
      "val  , epoch 135, 100%, acc: 9.1347e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch 136, 100%, acc: 9.9597e-01, loss: 1.4794e-02, lr: 4.0000e-03, took (s): 7.30\r\n",
      "val  , epoch 136, 100%, acc: 9.1288e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 137, 100%, acc: 9.9520e-01, loss: 1.5738e-02, lr: 4.0000e-03, took (s): 7.56\r\n",
      "val  , epoch 137, 100%, acc: 9.1090e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 138, 100%, acc: 9.9580e-01, loss: 1.5108e-02, lr: 4.0000e-03, took (s): 7.10\r\n",
      "val  , epoch 138, 100%, acc: 9.1119e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 139, 100%, acc: 9.9652e-01, loss: 1.3978e-02, lr: 4.0000e-03, took (s): 7.16\r\n",
      "val  , epoch 139, 100%, acc: 9.1021e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 140, 100%, acc: 9.9561e-01, loss: 1.5723e-02, lr: 4.0000e-03, took (s): 7.04\r\n",
      "val  , epoch 140, 100%, acc: 9.0922e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 141, 100%, acc: 9.9653e-01, loss: 1.3674e-02, lr: 4.0000e-03, took (s): 7.22\r\n",
      "val  , epoch 141, 100%, acc: 9.0932e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 142, 100%, acc: 9.9600e-01, loss: 1.4324e-02, lr: 4.0000e-03, took (s): 7.20\r\n",
      "val  , epoch 142, 100%, acc: 9.1100e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch 143, 100%, acc: 9.9666e-01, loss: 1.3054e-02, lr: 4.0000e-03, took (s): 7.33\r\n",
      "val  , epoch 143, 100%, acc: 9.1169e-01, took (s): 2.06\r\n",
      "\n",
      "train, epoch 144, 100%, acc: 9.9641e-01, loss: 1.3566e-02, lr: 4.0000e-03, took (s): 7.08\r\n",
      "val  , epoch 144, 100%, acc: 9.1248e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 145, 100%, acc: 9.9683e-01, loss: 1.2072e-02, lr: 4.0000e-03, took (s): 7.45\r\n",
      "val  , epoch 145, 100%, acc: 9.1169e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch 146, 100%, acc: 9.9636e-01, loss: 1.3051e-02, lr: 4.0000e-03, took (s): 7.33\r\n",
      "val  , epoch 146, 100%, acc: 9.1317e-01, took (s): 2.06\r\n",
      "\n",
      "train, epoch 147, 100%, acc: 9.9652e-01, loss: 1.3726e-02, lr: 4.0000e-03, took (s): 7.42\r\n",
      "val  , epoch 147, 100%, acc: 9.1515e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch 148, 100%, acc: 9.9542e-01, loss: 1.5097e-02, lr: 4.0000e-03, took (s): 7.44\r\n",
      "val  , epoch 148, 100%, acc: 9.1189e-01, took (s): 2.09\r\n",
      "\n",
      "train, epoch 149, 100%, acc: 9.9573e-01, loss: 1.4961e-02, lr: 4.0000e-03, took (s): 7.14\r\n",
      "val  , epoch 149, 100%, acc: 9.1288e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch 150, 100%, acc: 9.9561e-01, loss: 1.4856e-02, lr: 4.0000e-03, took (s): 7.23\r\n",
      "val  , epoch 150, 100%, acc: 9.1208e-01, took (s): 2.09\r\n",
      "\n",
      "train, epoch 151, 100%, acc: 9.9594e-01, loss: 1.4116e-02, lr: 4.0000e-03, took (s): 7.52\r\n",
      "val  , epoch 151, 100%, acc: 9.1030e-01, took (s): 2.06\r\n",
      "\n",
      "train, epoch 152, 100%, acc: 9.9646e-01, loss: 1.3529e-02, lr: 4.0000e-03, took (s): 7.42\r\n",
      "val  , epoch 152, 100%, acc: 9.1357e-01, took (s): 2.05\r\n",
      "\n",
      "train, epoch 153, 100%, acc: 9.9555e-01, loss: 1.5089e-02, lr: 4.0000e-03, took (s): 7.20\r\n",
      "val  , epoch 153, 100%, acc: 9.1278e-01, took (s): 2.07\r\n",
      "\n",
      "train, epoch 154, 100%, acc: 9.9594e-01, loss: 1.4213e-02, lr: 4.0000e-03, took (s): 7.06\r\n",
      "val  , epoch 154, 100%, acc: 9.1317e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 155, 100%, acc: 9.9689e-01, loss: 1.2507e-02, lr: 4.0000e-03, took (s): 7.05\r\n",
      "val  , epoch 155, 100%, acc: 9.1268e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 156, 100%, acc: 9.9569e-01, loss: 1.4929e-02, lr: 4.0000e-03, took (s): 7.05\r\n",
      "val  , epoch 156, 100%, acc: 9.1278e-01, took (s): 1.99\r\n",
      "\n",
      "train, epoch 157, 100%, acc: 9.9552e-01, loss: 1.6299e-02, lr: 4.0000e-03, took (s): 7.01\r\n",
      "val  , epoch 157, 100%, acc: 9.1169e-01, took (s): 2.05\r\n",
      "\n",
      "train, epoch 158, 100%, acc: 9.9596e-01, loss: 1.4744e-02, lr: 4.0000e-03, took (s): 7.05\r\n",
      "val  , epoch 158, 100%, acc: 9.1139e-01, took (s): 1.98\r\n",
      "\n",
      "train, epoch 159, 100%, acc: 9.9550e-01, loss: 1.5496e-02, lr: 4.0000e-03, took (s): 7.05\r\n",
      "val  , epoch 159, 100%, acc: 9.1080e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 160, 100%, acc: 9.9535e-01, loss: 1.5722e-02, lr: 4.0000e-03, took (s): 7.08\r\n",
      "val  , epoch 160, 100%, acc: 9.0951e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 161, 100%, acc: 9.9699e-01, loss: 1.2082e-02, lr: 8.0000e-04, took (s): 7.04\r\n",
      "val  , epoch 161, 100%, acc: 9.1357e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 162, 100%, acc: 9.9808e-01, loss: 8.2515e-03, lr: 8.0000e-04, took (s): 7.03\r\n",
      "val  , epoch 162, 100%, acc: 9.1475e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 163, 100%, acc: 9.9835e-01, loss: 7.2290e-03, lr: 8.0000e-04, took (s): 7.12\r\n",
      "val  , epoch 163, 100%, acc: 9.1426e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 164, 100%, acc: 9.9843e-01, loss: 7.1495e-03, lr: 8.0000e-04, took (s): 7.16\r\n",
      "val  , epoch 164, 100%, acc: 9.1723e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 165, 100%, acc: 9.9888e-01, loss: 5.8979e-03, lr: 8.0000e-04, took (s): 7.10\r\n",
      "val  , epoch 165, 100%, acc: 9.1742e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 166, 100%, acc: 9.9878e-01, loss: 5.8070e-03, lr: 8.0000e-04, took (s): 7.01\r\n",
      "val  , epoch 166, 100%, acc: 9.1693e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 167, 100%, acc: 9.9862e-01, loss: 6.1549e-03, lr: 8.0000e-04, took (s): 7.07\r\n",
      "val  , epoch 167, 100%, acc: 9.1802e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 168, 100%, acc: 9.9904e-01, loss: 5.1696e-03, lr: 8.0000e-04, took (s): 7.24\r\n",
      "val  , epoch 168, 100%, acc: 9.1812e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch 169, 100%, acc: 9.9892e-01, loss: 5.1561e-03, lr: 8.0000e-04, took (s): 7.07\r\n",
      "val  , epoch 169, 100%, acc: 9.1752e-01, took (s): 2.09\r\n",
      "\n",
      "train, epoch 170, 100%, acc: 9.9940e-01, loss: 4.5568e-03, lr: 8.0000e-04, took (s): 7.42\r\n",
      "val  , epoch 170, 100%, acc: 9.1634e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 171, 100%, acc: 9.9894e-01, loss: 5.0923e-03, lr: 8.0000e-04, took (s): 7.27\r\n",
      "val  , epoch 171, 100%, acc: 9.1653e-01, took (s): 1.98\r\n",
      "\n",
      "train, epoch 172, 100%, acc: 9.9930e-01, loss: 4.3039e-03, lr: 8.0000e-04, took (s): 6.98\r\n",
      "val  , epoch 172, 100%, acc: 9.1812e-01, took (s): 1.97\r\n",
      "\n",
      "train, epoch 173, 100%, acc: 9.9942e-01, loss: 4.2030e-03, lr: 8.0000e-04, took (s): 7.14\r\n",
      "val  , epoch 173, 100%, acc: 9.1742e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch 174, 100%, acc: 9.9909e-01, loss: 4.5955e-03, lr: 8.0000e-04, took (s): 7.01\r\n",
      "val  , epoch 174, 100%, acc: 9.1564e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 175, 100%, acc: 9.9918e-01, loss: 4.6878e-03, lr: 8.0000e-04, took (s): 7.07\r\n",
      "val  , epoch 175, 100%, acc: 9.1574e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 176, 100%, acc: 9.9914e-01, loss: 4.5964e-03, lr: 8.0000e-04, took (s): 6.99\r\n",
      "val  , epoch 176, 100%, acc: 9.1574e-01, took (s): 2.05\r\n",
      "\n",
      "train, epoch 177, 100%, acc: 9.9950e-01, loss: 4.3499e-03, lr: 8.0000e-04, took (s): 7.10\r\n",
      "val  , epoch 177, 100%, acc: 9.1782e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 178, 100%, acc: 9.9920e-01, loss: 4.4286e-03, lr: 8.0000e-04, took (s): 7.12\r\n",
      "val  , epoch 178, 100%, acc: 9.1624e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 179, 100%, acc: 9.9948e-01, loss: 3.9820e-03, lr: 8.0000e-04, took (s): 7.05\r\n",
      "val  , epoch 179, 100%, acc: 9.1762e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 180, 100%, acc: 9.9942e-01, loss: 3.6847e-03, lr: 8.0000e-04, took (s): 7.18\r\n",
      "val  , epoch 180, 100%, acc: 9.1663e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 181, 100%, acc: 9.9930e-01, loss: 4.3088e-03, lr: 8.0000e-04, took (s): 7.05\r\n",
      "val  , epoch 181, 100%, acc: 9.1762e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 182, 100%, acc: 9.9946e-01, loss: 3.6590e-03, lr: 8.0000e-04, took (s): 7.10\r\n",
      "val  , epoch 182, 100%, acc: 9.1772e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 183, 100%, acc: 9.9936e-01, loss: 3.7832e-03, lr: 8.0000e-04, took (s): 7.06\r\n",
      "val  , epoch 183, 100%, acc: 9.1802e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 184, 100%, acc: 9.9932e-01, loss: 4.0526e-03, lr: 8.0000e-04, took (s): 7.04\r\n",
      "val  , epoch 184, 100%, acc: 9.1663e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 185, 100%, acc: 9.9940e-01, loss: 3.7279e-03, lr: 8.0000e-04, took (s): 7.00\r\n",
      "val  , epoch 185, 100%, acc: 9.1584e-01, took (s): 1.98\r\n",
      "\n",
      "train, epoch 186, 100%, acc: 9.9954e-01, loss: 3.5445e-03, lr: 8.0000e-04, took (s): 7.22\r\n",
      "val  , epoch 186, 100%, acc: 9.1782e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 187, 100%, acc: 9.9952e-01, loss: 3.4207e-03, lr: 8.0000e-04, took (s): 7.24\r\n",
      "val  , epoch 187, 100%, acc: 9.1901e-01, took (s): 1.97\r\n",
      "\n",
      "train, epoch 188, 100%, acc: 9.9954e-01, loss: 3.5922e-03, lr: 8.0000e-04, took (s): 7.06\r\n",
      "val  , epoch 188, 100%, acc: 9.1733e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch 189, 100%, acc: 9.9938e-01, loss: 3.7385e-03, lr: 8.0000e-04, took (s): 7.00\r\n",
      "val  , epoch 189, 100%, acc: 9.1733e-01, took (s): 2.01\r\n",
      "\n",
      "train, epoch 190, 100%, acc: 9.9958e-01, loss: 3.6276e-03, lr: 8.0000e-04, took (s): 7.15\r\n",
      "val  , epoch 190, 100%, acc: 9.1802e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 191, 100%, acc: 9.9945e-01, loss: 3.5663e-03, lr: 8.0000e-04, took (s): 7.11\r\n",
      "val  , epoch 191, 100%, acc: 9.1911e-01, took (s): 2.03\r\n",
      "\n",
      "train, epoch 192, 100%, acc: 9.9972e-01, loss: 3.1360e-03, lr: 8.0000e-04, took (s): 7.00\r\n",
      "val  , epoch 192, 100%, acc: 9.1940e-01, took (s): 2.05\r\n",
      "\n",
      "train, epoch 193, 100%, acc: 9.9944e-01, loss: 3.5680e-03, lr: 8.0000e-04, took (s): 7.15\r\n",
      "val  , epoch 193, 100%, acc: 9.1911e-01, took (s): 2.02\r\n",
      "\n",
      "train, epoch 194, 100%, acc: 9.9970e-01, loss: 3.1365e-03, lr: 8.0000e-04, took (s): 7.00\r\n",
      "val  , epoch 194, 100%, acc: 9.1950e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 195, 100%, acc: 9.9956e-01, loss: 3.3116e-03, lr: 8.0000e-04, took (s): 7.06\r\n",
      "val  , epoch 195, 100%, acc: 9.1920e-01, took (s): 2.04\r\n",
      "\n",
      "train, epoch 196, 100%, acc: 9.9954e-01, loss: 3.2652e-03, lr: 8.0000e-04, took (s): 6.99\r\n",
      "val  , epoch 196, 100%, acc: 9.1861e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 197, 100%, acc: 9.9950e-01, loss: 3.5347e-03, lr: 8.0000e-04, took (s): 7.03\r\n",
      "val  , epoch 197, 100%, acc: 9.1644e-01, took (s): 1.98\r\n",
      "\n",
      "train, epoch 198, 100%, acc: 9.9942e-01, loss: 3.4536e-03, lr: 8.0000e-04, took (s): 7.04\r\n",
      "val  , epoch 198, 100%, acc: 9.1812e-01, took (s): 1.98\r\n",
      "\n",
      "train, epoch 199, 100%, acc: 9.9964e-01, loss: 2.9916e-03, lr: 8.0000e-04, took (s): 6.99\r\n",
      "val  , epoch 199, 100%, acc: 9.1812e-01, took (s): 2.00\r\n",
      "\n",
      "train, epoch 200, 100%, acc: 9.9960e-01, loss: 3.3930e-03, lr: 8.0000e-04, took (s): 7.07\r\n",
      "val  , epoch 200, 100%, acc: 9.1831e-01, took (s): 2.04\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(args.nb_epochs):\n",
    "\ttrain(e)\n",
    "\twith torch.no_grad():\n",
    "\t\tval(e)\n",
    "\tif scheduler is not None:\n",
    "\t\tscheduler.step()\n",
    "\tprint(\"\")\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}