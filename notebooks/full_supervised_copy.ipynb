{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/labbeti/anaconda3/envs/env_ssl/bin/python'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "source": [
    "# Utility functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "\n",
    "\n",
    "def get_datetime():\n",
    "    now = datetime.datetime.now()\n",
    "    return str(now)[:10] + \"_\" + str(now)[11:-7]\n",
    "\n",
    "\n",
    "def reset_seed(seed):\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    numpy.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"SpeechCommand\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"cnn03\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=1.0, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=128, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--preload_dataset\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=10, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[5], type=int)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../results/model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../results/tensorboard_leo/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Namespace(batch_size=128, checkpoint_path='supervised', checkpoint_root='../results/model_save/', dataset='SpeechCommand', dataset_root='../datasets', from_config='', learning_rate=0.1, model='cnn03', nb_epoch=200, num_classes=10, preload_dataset=False, resume=False, seed=1234, supervised_ratio=1.0, tensorboard_path='supervised', tensorboard_root='../results/tensorboard_leo/', tensorboard_sufix='', train_folds=[1, 2, 3, 4], val_folds=[5])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(0, translate=(1/16, 1/16)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_s_u(train_dataset, s_ratio: float = 0.08, nb_class: int = 10):\n",
    "    if s_ratio == 1.0:\n",
    "        return list(range(len(train_dataset))), []\n",
    "\n",
    "    s_idx, u_idx = [], []\n",
    "    nb_s = int(np.ceil(len(train_dataset) * s_ratio) // nb_class)\n",
    "    cls_idx = [[] for _ in range(nb_class)]\n",
    "\n",
    "    # To each file, an index is assigned, then they are split into classes\n",
    "    for i in range(len(train_dataset)):\n",
    "        _, y = train_dataset[i]\n",
    "        cls_idx[y].append(i)\n",
    "\n",
    "    for i in range(len(cls_idx)):\n",
    "        random.shuffle(cls_idx[i])\n",
    "        s_idx += cls_idx[i][:nb_s]\n",
    "        u_idx += cls_idx[i][nb_s:]\n",
    "\n",
    "    return s_idx, u_idx\n",
    "\n",
    "\n",
    "def load_supervised(\n",
    "        dataset_root,\n",
    "        supervised_ratio: float = 1.0,\n",
    "        batch_size: int = 128,\n",
    "        train_transform: list = [],\n",
    "        val_transform: list = [],\n",
    "        **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the cifar10 dataset for Deep Co Training system.\n",
    "    \"\"\"\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=os.path.join(dataset_root, \"CIFAR10\"), train=True, download=True, transform=train_transform)\n",
    "    val_dataset = torchvision.datasets.CIFAR10(root=os.path.join(dataset_root, \"CIFAR10\"), train=False, download=True, transform=val_transform)\n",
    "\n",
    "    # Split the training dataset into a supervised and unsupervised sets\n",
    "    s_idx, u_idx = _split_s_u(train_dataset, supervised_ratio)\n",
    "\n",
    "    sampler_s1 = data.SubsetRandomSampler(s_idx)\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, sampler=sampler_s1, num_workers=4, pin_memory=True, )\n",
    "    val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, )\n",
    "\n",
    "    return None, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_supervised(\n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "source": [
    "## modification of pytorch wideresnet implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.expansion = 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, layers, width: int = 2, num_classes=10, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=16, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        block = BasicBlock\n",
    "        self.inplanes = 16*width\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 16*width, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32*width, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 64*width, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64* width * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                \n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wideresnet28_2(**kwargs):\n",
    "    return ResNet([4, 4, 4], num_classes=10)"
   ]
  },
  {
   "source": [
    "## Create the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = wideresnet28_2\n",
    "\n",
    "model = model_func(input_shape=input_shape, num_classes = args.num_classes)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             864\n",
      "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
      "              ReLU-3           [-1, 32, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
      "            Conv2d-5           [-1, 32, 16, 16]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
      "              ReLU-7           [-1, 32, 16, 16]               0\n",
      "            Conv2d-8           [-1, 32, 16, 16]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
      "             ReLU-10           [-1, 32, 16, 16]               0\n",
      "       BasicBlock-11           [-1, 32, 16, 16]               0\n",
      "           Conv2d-12           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-13           [-1, 32, 16, 16]              64\n",
      "             ReLU-14           [-1, 32, 16, 16]               0\n",
      "           Conv2d-15           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-16           [-1, 32, 16, 16]              64\n",
      "             ReLU-17           [-1, 32, 16, 16]               0\n",
      "       BasicBlock-18           [-1, 32, 16, 16]               0\n",
      "           Conv2d-19           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-20           [-1, 32, 16, 16]              64\n",
      "             ReLU-21           [-1, 32, 16, 16]               0\n",
      "           Conv2d-22           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-23           [-1, 32, 16, 16]              64\n",
      "             ReLU-24           [-1, 32, 16, 16]               0\n",
      "       BasicBlock-25           [-1, 32, 16, 16]               0\n",
      "           Conv2d-26           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-27           [-1, 32, 16, 16]              64\n",
      "             ReLU-28           [-1, 32, 16, 16]               0\n",
      "           Conv2d-29           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-30           [-1, 32, 16, 16]              64\n",
      "             ReLU-31           [-1, 32, 16, 16]               0\n",
      "       BasicBlock-32           [-1, 32, 16, 16]               0\n",
      "           Conv2d-33             [-1, 64, 8, 8]          18,432\n",
      "      BatchNorm2d-34             [-1, 64, 8, 8]             128\n",
      "             ReLU-35             [-1, 64, 8, 8]               0\n",
      "           Conv2d-36             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-37             [-1, 64, 8, 8]             128\n",
      "           Conv2d-38             [-1, 64, 8, 8]           2,048\n",
      "      BatchNorm2d-39             [-1, 64, 8, 8]             128\n",
      "             ReLU-40             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-41             [-1, 64, 8, 8]               0\n",
      "           Conv2d-42             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-43             [-1, 64, 8, 8]             128\n",
      "             ReLU-44             [-1, 64, 8, 8]               0\n",
      "           Conv2d-45             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-46             [-1, 64, 8, 8]             128\n",
      "             ReLU-47             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-48             [-1, 64, 8, 8]               0\n",
      "           Conv2d-49             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-50             [-1, 64, 8, 8]             128\n",
      "             ReLU-51             [-1, 64, 8, 8]               0\n",
      "           Conv2d-52             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-53             [-1, 64, 8, 8]             128\n",
      "             ReLU-54             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-55             [-1, 64, 8, 8]               0\n",
      "           Conv2d-56             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-57             [-1, 64, 8, 8]             128\n",
      "             ReLU-58             [-1, 64, 8, 8]               0\n",
      "           Conv2d-59             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-60             [-1, 64, 8, 8]             128\n",
      "             ReLU-61             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-62             [-1, 64, 8, 8]               0\n",
      "           Conv2d-63            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-64            [-1, 128, 4, 4]             256\n",
      "             ReLU-65            [-1, 128, 4, 4]               0\n",
      "           Conv2d-66            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-67            [-1, 128, 4, 4]             256\n",
      "           Conv2d-68            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-69            [-1, 128, 4, 4]             256\n",
      "             ReLU-70            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-71            [-1, 128, 4, 4]               0\n",
      "           Conv2d-72            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
      "             ReLU-74            [-1, 128, 4, 4]               0\n",
      "           Conv2d-75            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-76            [-1, 128, 4, 4]             256\n",
      "             ReLU-77            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-78            [-1, 128, 4, 4]               0\n",
      "           Conv2d-79            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-80            [-1, 128, 4, 4]             256\n",
      "             ReLU-81            [-1, 128, 4, 4]               0\n",
      "           Conv2d-82            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-83            [-1, 128, 4, 4]             256\n",
      "             ReLU-84            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-85            [-1, 128, 4, 4]               0\n",
      "           Conv2d-86            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-87            [-1, 128, 4, 4]             256\n",
      "             ReLU-88            [-1, 128, 4, 4]               0\n",
      "           Conv2d-89            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-90            [-1, 128, 4, 4]             256\n",
      "             ReLU-91            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-92            [-1, 128, 4, 4]               0\n",
      "AdaptiveAvgPool2d-93            [-1, 128, 1, 1]               0\n",
      "           Linear-94                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 1,472,554\n",
      "Trainable params: 1,472,554\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.97\n",
      "Params size (MB): 5.62\n",
      "Estimated Total Size (MB): 9.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "s = summary(model, input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/tensorboard_leo/SpeechCommand/supervised/cnn03/1.0S/2020-12-14_16:52:57_wideresnet28_2_1.0S\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "title_element = (args.model, args.supervised_ratio, get_datetime(), model_func.__name__, args.supervised_ratio)\n",
    "tensorboard_title = \"%s/%sS/%s_%s_%.1fS\" % title_element\n",
    "\n",
    "title_element = (model_func.__name__, args.supervised_ratio)\n",
    "checkpoint_title = \"%s_%.1fS\" % title_element\n",
    "\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))\n",
    "\n",
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_params = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    tensorboard_params[key] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard.add_hparams(tensorboard_params, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "source": [
    "## callbacks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(e):\n",
    "    if e < 60:\n",
    "        return 1\n",
    "\n",
    "    elif 60 <= e < 120:\n",
    "        return 0.2\n",
    "\n",
    "    elif 120 <= e < 160:\n",
    "        return 0.04\n",
    "\n",
    "    else:\n",
    "        return 0.008\n",
    "\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "callbacks = [lr_scheduler]"
   ]
  },
  {
   "source": [
    "## Checkpoint and metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "# checkpoint = CheckPoint(model, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "# Metrics\n",
    "fscore_fn = FScore()\n",
    "acc_fn = CategoricalAccuracy()\n",
    "avg = ContinueAverage()\n",
    "\n",
    "maximum_tracker = track_maximum()\n",
    "\n",
    "reset_metrics = lambda : [m.reset() for m in [fscore_fn, acc_fn, avg]]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} - {:<9.9} {:<12.12}| {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} - {:<9.9} {:<10.4f}| {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \".               \", \"Epoch\", \"%\", \"Losses:\", \"ce\", \"metrics: \", \"acc\", \"F1 \",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    model.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(train_loader):        \n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        logits = model(X)        \n",
    "        loss = loss_ce(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y)  # .mean\n",
    "            fscore = fscore_fn(pred, y_one_hot)  # .mean\n",
    "            avg_ce = avg(loss.item())  # .mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(train_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"train/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_ce(logits, y)\n",
    "\n",
    "            # metrics\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y)  # .mean\n",
    "            fscore = fscore_fn(pred, y_one_hot)  # .mean\n",
    "            avg_ce = avg(loss.item())  # .mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"val/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc\", acc, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/acc\", maximum_tracker(\"acc\", acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/f1\", maximum_tracker(\"f1\", fscore), epoch )\n",
    "\n",
    "    # checkpoint.step(acc)\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n",
      "\n",
      "Training 1      - 100    -          1.6960 -           0.3759    | 0.2300   - 7.3532\r\n",
      "\u001B[1;4mValidati 1      - 100    -          1.6200 -           0.4131    | 0.3272   - 0.4775\u001B[0m\r\n",
      "Training 2      - 100    -          1.2157 -           0.5621    | 0.5113   - 7.3837\r\n",
      "\u001B[1;4mValidati 2      - 100    -          1.3933 -           0.5247    | 0.5034   - 0.4665\u001B[0m\r\n",
      "Training 3      - 100    -          1.0075 -           0.6451    | 0.6212   - 7.2352\r\n",
      "\u001B[1;4mValidati 3      - 100    -          1.1222 -           0.5956    | 0.5796   - 0.4756\u001B[0m\r\n",
      "Training 4      - 100    -          0.8781 -           0.6934    | 0.6804   - 7.4696\r\n",
      "\u001B[1;4mValidati 4      - 100    -          1.2931 -           0.5295    | 0.5133   - 0.4682\u001B[0m\r\n",
      "Training 5      - 100    -          0.7682 -           0.7331    | 0.7263   - 7.2402\r\n",
      "\u001B[1;4mValidati 5      - 100    -          1.0427 -           0.6476    | 0.6463   - 0.4872\u001B[0m\r\n",
      "Training 6      - 100    -          0.7103 -           0.7546    | 0.7499   - 7.3861\r\n",
      "\u001B[1;4mValidati 6      - 100    -          1.1639 -           0.6317    | 0.6310   - 0.4693\u001B[0m\r\n",
      "Training 7      - 100    -          0.6581 -           0.7729    | 0.7694   - 7.4163\r\n",
      "\u001B[1;4mValidati 7      - 100    -          1.3417 -           0.6143    | 0.6146   - 0.4601\u001B[0m\r\n",
      "Training 8      - 100    -          0.6379 -           0.7811    | 0.7784   - 7.4300\r\n",
      "\u001B[1;4mValidati 8      - 100    -          0.9840 -           0.6777    | 0.6757   - 0.4780\u001B[0m\r\n",
      "Training 9      - 100    -          0.6101 -           0.7906    | 0.7881   - 7.5110\r\n",
      "\u001B[1;4mValidati 9      - 100    -          0.8720 -           0.7108    | 0.7089   - 0.4536\u001B[0m\r\n",
      "Training 10     - 100    -          0.5952 -           0.7963    | 0.7947   - 7.4117\r\n",
      "\u001B[1;4mValidati 10     - 100    -          0.7482 -           0.7444    | 0.7439   - 0.4610\u001B[0m\r\n",
      "Training 11     - 100    -          0.5774 -           0.8009    | 0.8005   - 6.9666\r\n",
      "\u001B[1;4mValidati 11     - 100    -          0.9631 -           0.6896    | 0.6877   - 0.4512\u001B[0m\r\n",
      "Training 12     - 100    -          0.5572 -           0.8082    | 0.8072   - 6.8602\r\n",
      "\u001B[1;4mValidati 12     - 100    -          0.7280 -           0.7513    | 0.7509   - 0.4158\u001B[0m\r\n",
      "Training 13     - 100    -          0.5531 -           0.8092    | 0.8082   - 6.9205\r\n",
      "\u001B[1;4mValidati 13     - 100    -          0.6912 -           0.7649    | 0.7629   - 0.4364\u001B[0m\r\n",
      "Training 14     - 100    -          0.5432 -           0.8132    | 0.8118   - 7.0141\r\n",
      "\u001B[1;4mValidati 14     - 100    -          0.6794 -           0.7688    | 0.7723   - 0.4483\u001B[0m\r\n",
      "Training 15     - 100    -          0.5397 -           0.8154    | 0.8143   - 6.9911\r\n",
      "\u001B[1;4mValidati 15     - 100    -          0.6704 -           0.7730    | 0.7741   - 0.4394\u001B[0m\r\n",
      "Training 16     - 100    -          0.5211 -           0.8209    | 0.8203   - 6.8203\r\n",
      "\u001B[1;4mValidati 16     - 100    -          0.7983 -           0.7390    | 0.7409   - 0.4250\u001B[0m\r\n",
      "Training 17     - 100    -          0.5209 -           0.8218    | 0.8219   - 7.0048\r\n",
      "\u001B[1;4mValidati 17     - 100    -          0.8453 -           0.7059    | 0.7089   - 0.4435\u001B[0m\r\n",
      "Training 18     - 100    -          0.5187 -           0.8228    | 0.8225   - 6.8468\r\n",
      "\u001B[1;4mValidati 18     - 100    -          0.9029 -           0.6952    | 0.6933   - 0.4433\u001B[0m\r\n",
      "Training 19     - 100    -          0.5047 -           0.8258    | 0.8255   - 7.0348\r\n",
      "\u001B[1;4mValidati 19     - 100    -          0.7179 -           0.7565    | 0.7596   - 0.4372\u001B[0m\r\n",
      "Training 20     - 100    -          0.5062 -           0.8268    | 0.8266   - 6.9801\r\n",
      "\u001B[1;4mValidati 20     - 100    -          0.8943 -           0.7111    | 0.7123   - 0.4181\u001B[0m\r\n",
      "Training 21     - 100    -          0.4985 -           0.8290    | 0.8283   - 6.8504\r\n",
      "\u001B[1;4mValidati 21     - 100    -          0.6324 -           0.7887    | 0.7884   - 0.4488\u001B[0m\r\n",
      "Training 22     - 100    -          0.4915 -           0.8313    | 0.8318   - 7.0498\r\n",
      "\u001B[1;4mValidati 22     - 100    -          1.3122 -           0.6083    | 0.6128   - 0.4218\u001B[0m\r\n",
      "Training 23     - 100    -          0.4947 -           0.8296    | 0.8308   - 6.9436\r\n",
      "\u001B[1;4mValidati 23     - 100    -          0.6657 -           0.7661    | 0.7691   - 0.4184\u001B[0m\r\n",
      "Training 24     - 100    -          0.4895 -           0.8322    | 0.8316   - 6.9150\r\n",
      "\u001B[1;4mValidati 24     - 100    -          0.9288 -           0.7047    | 0.7031   - 0.4440\u001B[0m\r\n",
      "Training 25     - 100    -          0.4910 -           0.8318    | 0.8319   - 6.8741\r\n",
      "\u001B[1;4mValidati 25     - 100    -          0.6679 -           0.7769    | 0.7781   - 0.4178\u001B[0m\r\n",
      "Training 26     - 100    -          0.4805 -           0.8353    | 0.8351   - 7.0129\r\n",
      "\u001B[1;4mValidati 26     - 100    -          0.7921 -           0.7424    | 0.7451   - 0.4237\u001B[0m\r\n",
      "Training 27     - 100    -          0.4747 -           0.8400    | 0.8397   - 6.9484\r\n",
      "\u001B[1;4mValidati 27     - 100    -          0.7071 -           0.7642    | 0.7652   - 0.4141\u001B[0m\r\n",
      "Training 28     - 100    -          0.4725 -           0.8374    | 0.8371   - 6.9659\r\n",
      "\u001B[1;4mValidati 28     - 100    -          0.6256 -           0.7864    | 0.7894   - 0.4290\u001B[0m\r\n",
      "Training 29     - 100    -          0.4777 -           0.8350    | 0.8352   - 6.9530\r\n",
      "\u001B[1;4mValidati 29     - 100    -          0.8458 -           0.7216    | 0.7239   - 0.4401\u001B[0m\r\n",
      "Training 30     - 100    -          0.4766 -           0.8376    | 0.8378   - 6.9685\r\n",
      "\u001B[1;4mValidati 30     - 100    -          0.6251 -           0.7869    | 0.7867   - 0.4334\u001B[0m\r\n",
      "Training 31     - 100    -          0.4643 -           0.8406    | 0.8410   - 6.8765\r\n",
      "\u001B[1;4mValidati 31     - 100    -          0.9338 -           0.7065    | 0.7072   - 0.4502\u001B[0m\r\n",
      "Training 32     - 100    -          0.4658 -           0.8400    | 0.8403   - 6.9742\r\n",
      "\u001B[1;4mValidati 32     - 100    -          0.6919 -           0.7703    | 0.7732   - 0.4139\u001B[0m\r\n",
      "Training 33     - 100    -          0.4678 -           0.8398    | 0.8400   - 6.8983\r\n",
      "\u001B[1;4mValidati 33     - 100    -          0.6477 -           0.7831    | 0.7870   - 0.4378\u001B[0m\r\n",
      "Training 34     - 100    -          0.4624 -           0.8419    | 0.8422   - 6.9792\r\n",
      "\u001B[1;4mValidati 34     - 100    -          1.1161 -           0.6794    | 0.6859   - 0.4784\u001B[0m\r\n",
      "Training 35     - 100    -          0.4638 -           0.8412    | 0.8419   - 6.9720\r\n",
      "\u001B[1;4mValidati 35     - 100    -          0.9759 -           0.6845    | 0.6866   - 0.4270\u001B[0m\r\n",
      "Training 36     - 100    -          0.4665 -           0.8403    | 0.8414   - 6.9495\r\n",
      "\u001B[1;4mValidati 36     - 100    -          0.7992 -           0.7395    | 0.7403   - 0.4117\u001B[0m\r\n",
      "Training 37     - 100    -          0.4589 -           0.8417    | 0.8438   - 7.1394\r\n",
      "\u001B[1;4mValidati 37     - 100    -          1.1324 -           0.6663    | 0.6687   - 0.4451\u001B[0m\r\n",
      "Training 38     - 100    -          0.4656 -           0.8404    | 0.8420   - 6.9431\r\n",
      "\u001B[1;4mValidati 38     - 100    -          0.5894 -           0.8004    | 0.8024   - 0.4264\u001B[0m\r\n",
      "Training 39     - 100    -          0.4610 -           0.8419    | 0.8425   - 6.9243\r\n",
      "\u001B[1;4mValidati 39     - 100    -          0.9794 -           0.6841    | 0.6836   - 0.4300\u001B[0m\r\n",
      "Training 40     - 100    -          0.4599 -           0.8417    | 0.8437   - 6.8811\r\n",
      "\u001B[1;4mValidati 40     - 100    -          0.5067 -           0.8316    | 0.8320   - 0.4107\u001B[0m\r\n",
      "Training 41     - 100    -          0.4540 -           0.8438    | 0.8446   - 6.9548\r\n",
      "\u001B[1;4mValidati 41     - 100    -          0.7175 -           0.7587    | 0.7586   - 0.4530\u001B[0m\r\n",
      "Training 42     - 100    -          0.4453 -           0.8489    | 0.8495   - 6.9595\r\n",
      "\u001B[1;4mValidati 42     - 100    -          0.7542 -           0.7587    | 0.7595   - 0.4287\u001B[0m\r\n",
      "Training 43     - 100    -          0.4543 -           0.8458    | 0.8451   - 6.9760\r\n",
      "\u001B[1;4mValidati 43     - 100    -          0.7014 -           0.7707    | 0.7718   - 0.4502\u001B[0m\r\n",
      "Training 44     - 100    -          0.4552 -           0.8437    | 0.8446   - 6.9959\r\n",
      "\u001B[1;4mValidati 44     - 100    -          0.6484 -           0.7813    | 0.7811   - 0.4241\u001B[0m\r\n",
      "Training 45     - 100    -          0.4500 -           0.8468    | 0.8478   - 6.9937\r\n",
      "\u001B[1;4mValidati 45     - 100    -          0.5699 -           0.8098    | 0.8109   - 0.4344\u001B[0m\r\n",
      "Training 46     - 100    -          0.4488 -           0.8462    | 0.8470   - 6.9377\r\n",
      "\u001B[1;4mValidati 46     - 100    -          0.6140 -           0.7951    | 0.7928   - 0.4198\u001B[0m\r\n",
      "Training 47     - 100    -          0.4516 -           0.8452    | 0.8452   - 7.0789\r\n",
      "\u001B[1;4mValidati 47     - 100    -          0.7142 -           0.7574    | 0.7574   - 0.4349\u001B[0m\r\n",
      "Training 48     - 100    -          0.4490 -           0.8451    | 0.8465   - 7.0664\r\n",
      "\u001B[1;4mValidati 48     - 100    -          0.9061 -           0.7050    | 0.7052   - 0.4373\u001B[0m\r\n",
      "Training 49     - 100    -          0.4481 -           0.8457    | 0.8475   - 6.8462\r\n",
      "\u001B[1;4mValidati 49     - 100    -          0.7517 -           0.7377    | 0.7370   - 0.4643\u001B[0m\r\n",
      "Training 50     - 100    -          0.4441 -           0.8491    | 0.8494   - 7.4036\r\n",
      "\u001B[1;4mValidati 50     - 100    -          0.7666 -           0.7563    | 0.7592   - 0.4248\u001B[0m\r\n",
      "Training 51     - 100    -          0.4500 -           0.8452    | 0.8463   - 7.1939\r\n",
      "\u001B[1;4mValidati 51     - 100    -          0.7056 -           0.7645    | 0.7710   - 0.4284\u001B[0m\r\n",
      "Training 52     - 100    -          0.4490 -           0.8449    | 0.8462   - 7.2635\r\n",
      "\u001B[1;4mValidati 52     - 100    -          0.8199 -           0.7262    | 0.7268   - 0.4354\u001B[0m\r\n",
      "Training 53     - 100    -          0.4482 -           0.8453    | 0.8463   - 7.3185\r\n",
      "\u001B[1;4mValidati 53     - 100    -          0.6578 -           0.7766    | 0.7780   - 0.4261\u001B[0m\r\n",
      "Training 54     - 100    -          0.4403 -           0.8474    | 0.8485   - 7.2081\r\n",
      "\u001B[1;4mValidati 54     - 100    -          0.6153 -           0.7926    | 0.7997   - 0.4403\u001B[0m\r\n",
      "Training 55     - 100    -          0.4442 -           0.8472    | 0.8481   - 7.0160\r\n",
      "\u001B[1;4mValidati 55     - 100    -          0.7926 -           0.7360    | 0.7355   - 0.4893\u001B[0m\r\n",
      "Training 56     - 100    -          0.4472 -           0.8465    | 0.8468   - 7.0905\r\n",
      "\u001B[1;4mValidati 56     - 100    -          0.7957 -           0.7516    | 0.7484   - 0.4455\u001B[0m\r\n",
      "Training 57     - 100    -          0.4447 -           0.8465    | 0.8465   - 7.1193\r\n",
      "\u001B[1;4mValidati 57     - 100    -          0.7804 -           0.7394    | 0.7439   - 0.4682\u001B[0m\r\n",
      "Training 58     - 100    -          0.4423 -           0.8481    | 0.8489   - 7.1767\r\n",
      "\u001B[1;4mValidati 58     - 100    -          0.7726 -           0.7414    | 0.7422   - 0.4538\u001B[0m\r\n",
      "Training 59     - 100    -          0.4388 -           0.8496    | 0.8506   - 7.0494\r\n",
      "\u001B[1;4mValidati 59     - 100    -          0.7142 -           0.7724    | 0.7759   - 0.4574\u001B[0m\r\n",
      "Training 60     - 100    -          0.4483 -           0.8467    | 0.8475   - 7.2213\r\n",
      "\u001B[1;4mValidati 60     - 100    -          0.7475 -           0.7511    | 0.7530   - 0.4732\u001B[0m\r\n",
      "Training 61     - 100    -          0.2699 -           0.9089    | 0.9097   - 7.1265\r\n",
      "\u001B[1;4mValidati 61     - 100    -          0.3686 -           0.8770    | 0.8799   - 0.4278\u001B[0m\r\n",
      "Training 62     - 100    -          0.2200 -           0.9247    | 0.9252   - 7.1134\r\n",
      "\u001B[1;4mValidati 62     - 100    -          0.3351 -           0.8881    | 0.8901   - 0.4682\u001B[0m\r\n",
      "Training 63     - 100    -          0.1994 -           0.9320    | 0.9324   - 7.1721\r\n",
      "\u001B[1;4mValidati 63     - 100    -          0.3490 -           0.8877    | 0.8887   - 0.4466\u001B[0m\r\n",
      "Training 64     - 100    -          0.1900 -           0.9339    | 0.9348   - 7.1673\r\n",
      "\u001B[1;4mValidati 64     - 100    -          0.3285 -           0.8941    | 0.8953   - 0.4328\u001B[0m\r\n",
      "Training 65     - 100    -          0.1801 -           0.9377    | 0.9385   - 7.0323\r\n",
      "\u001B[1;4mValidati 65     - 100    -          0.3492 -           0.8870    | 0.8877   - 0.4589\u001B[0m\r\n",
      "Training 66     - 100    -          0.1735 -           0.9400    | 0.9409   - 7.1192\r\n",
      "\u001B[1;4mValidati 66     - 100    -          0.3565 -           0.8813    | 0.8838   - 0.4280\u001B[0m\r\n",
      "Training 67     - 100    -          0.1705 -           0.9406    | 0.9414   - 7.2299\r\n",
      "\u001B[1;4mValidati 67     - 100    -          0.3972 -           0.8740    | 0.8748   - 0.4686\u001B[0m\r\n",
      "Training 68     - 100    -          0.1643 -           0.9424    | 0.9436   - 7.2344\r\n",
      "\u001B[1;4mValidati 68     - 100    -          0.3579 -           0.8870    | 0.8884   - 0.4690\u001B[0m\r\n",
      "Training 69     - 100    -          0.1696 -           0.9408    | 0.9410   - 7.2455\r\n",
      "\u001B[1;4mValidati 69     - 100    -          0.4345 -           0.8646    | 0.8655   - 0.4322\u001B[0m\r\n",
      "Training 70     - 100    -          0.1705 -           0.9400    | 0.9407   - 7.1478\r\n",
      "\u001B[1;4mValidati 70     - 100    -          0.4265 -           0.8652    | 0.8687   - 0.4531\u001B[0m\r\n",
      "Training 71     - 100    -          0.1730 -           0.9397    | 0.9404   - 7.2768\r\n",
      "\u001B[1;4mValidati 71     - 100    -          0.4143 -           0.8708    | 0.8733   - 0.4306\u001B[0m\r\n",
      "Training 72     - 100    -          0.1739 -           0.9399    | 0.9409   - 7.2105\r\n",
      "\u001B[1;4mValidati 72     - 100    -          0.3857 -           0.8788    | 0.8794   - 0.4470\u001B[0m\r\n",
      "Training 73     - 100    -          0.1764 -           0.9385    | 0.9393   - 7.1702\r\n",
      "\u001B[1;4mValidati 73     - 100    -          0.3885 -           0.8768    | 0.8782   - 0.4566\u001B[0m\r\n",
      "Training 74     - 100    -          0.1745 -           0.9379    | 0.9389   - 7.3040\r\n",
      "\u001B[1;4mValidati 74     - 100    -          0.4745 -           0.8528    | 0.8549   - 0.4276\u001B[0m\r\n",
      "Training 75     - 100    -          0.1786 -           0.9378    | 0.9383   - 7.0910\r\n",
      "\u001B[1;4mValidati 75     - 100    -          0.4226 -           0.8627    | 0.8659   - 0.4580\u001B[0m\r\n",
      "Training 76     - 100    -          0.1741 -           0.9384    | 0.9388   - 7.1081\r\n",
      "\u001B[1;4mValidati 76     - 100    -          0.4195 -           0.8665    | 0.8695   - 0.4313\u001B[0m\r\n",
      "Training 77     - 100    -          0.1738 -           0.9400    | 0.9404   - 7.1707\r\n",
      "\u001B[1;4mValidati 77     - 100    -          0.3963 -           0.8731    | 0.8754   - 0.4277\u001B[0m\r\n",
      "Training 78     - 100    -          0.1781 -           0.9379    | 0.9387   - 7.2254\r\n",
      "\u001B[1;4mValidati 78     - 100    -          0.4404 -           0.8565    | 0.8600   - 0.4221\u001B[0m\r\n",
      "Training 79     - 100    -          0.1713 -           0.9389    | 0.9399   - 7.1303\r\n",
      "\u001B[1;4mValidati 79     - 100    -          0.4609 -           0.8591    | 0.8612   - 0.4369\u001B[0m\r\n",
      "Training 80     - 100    -          0.1746 -           0.9388    | 0.9391   - 7.1240\r\n",
      "\u001B[1;4mValidati 80     - 100    -          0.4064 -           0.8729    | 0.8738   - 0.4270\u001B[0m\r\n",
      "Training 81     - 100    -          0.1730 -           0.9408    | 0.9411   - 7.1176\r\n",
      "\u001B[1;4mValidati 81     - 100    -          0.4383 -           0.8616    | 0.8637   - 0.4686\u001B[0m\r\n",
      "Training 82     - 100    -          0.1755 -           0.9378    | 0.9385   - 7.1838\r\n",
      "\u001B[1;4mValidati 82     - 100    -          0.4584 -           0.8607    | 0.8641   - 0.4476\u001B[0m\r\n",
      "Training 83     - 100    -          0.1680 -           0.9414    | 0.9420   - 7.2091\r\n",
      "\u001B[1;4mValidati 83     - 100    -          0.4322 -           0.8664    | 0.8672   - 0.4732\u001B[0m\r\n",
      "Training 84     - 100    -          0.1686 -           0.9420    | 0.9421   - 7.2216\r\n",
      "\u001B[1;4mValidati 84     - 100    -          0.5034 -           0.8480    | 0.8500   - 0.4517\u001B[0m\r\n",
      "Training 85     - 100    -          0.1754 -           0.9386    | 0.9389   - 7.1458\r\n",
      "\u001B[1;4mValidati 85     - 100    -          0.5204 -           0.8342    | 0.8358   - 0.4392\u001B[0m\r\n",
      "Training 86     - 100    -          0.1714 -           0.9413    | 0.9416   - 7.0977\r\n",
      "\u001B[1;4mValidati 86     - 100    -          0.5117 -           0.8502    | 0.8520   - 0.4849\u001B[0m\r\n",
      "Training 87     - 100    -          0.1661 -           0.9411    | 0.9421   - 7.3508\r\n",
      "\u001B[1;4mValidati 87     - 100    -          0.7251 -           0.7993    | 0.8009   - 0.4327\u001B[0m\r\n",
      "Training 88     - 100    -          0.1689 -           0.9402    | 0.9410   - 6.9231\r\n",
      "\u001B[1;4mValidati 88     - 100    -          0.5116 -           0.8451    | 0.8475   - 0.4454\u001B[0m\r\n",
      "Training 89     - 100    -          0.1660 -           0.9422    | 0.9421   - 7.0142\r\n",
      "\u001B[1;4mValidati 89     - 100    -          0.4056 -           0.8752    | 0.8766   - 0.4227\u001B[0m\r\n",
      "Training 90     - 100    -          0.1679 -           0.9409    | 0.9417   - 7.0487\r\n",
      "\u001B[1;4mValidati 90     - 100    -          0.7089 -           0.8029    | 0.8054   - 0.4679\u001B[0m\r\n",
      "Training 91     - 100    -          0.1702 -           0.9402    | 0.9406   - 6.9345\r\n",
      "\u001B[1;4mValidati 91     - 100    -          0.4470 -           0.8626    | 0.8661   - 0.4370\u001B[0m\r\n",
      "Training 92     - 100    -          0.1619 -           0.9441    | 0.9439   - 7.0233\r\n",
      "\u001B[1;4mValidati 92     - 100    -          0.4868 -           0.8571    | 0.8597   - 0.4616\u001B[0m\r\n",
      "Training 93     - 100    -          0.1663 -           0.9418    | 0.9426   - 7.1737\r\n",
      "\u001B[1;4mValidati 93     - 100    -          0.4951 -           0.8496    | 0.8534   - 0.4682\u001B[0m\r\n",
      "Training 94     - 100    -          0.1672 -           0.9411    | 0.9412   - 7.1415\r\n",
      "\u001B[1;4mValidati 94     - 100    -          0.4401 -           0.8613    | 0.8659   - 0.4506\u001B[0m\r\n",
      "Training 95     - 100    -          0.1625 -           0.9440    | 0.9439   - 7.1383\r\n",
      "\u001B[1;4mValidati 95     - 100    -          0.4583 -           0.8609    | 0.8635   - 0.4415\u001B[0m\r\n",
      "Training 96     - 100    -          0.1642 -           0.9433    | 0.9434   - 7.0710\r\n",
      "\u001B[1;4mValidati 96     - 100    -          0.4175 -           0.8719    | 0.8736   - 0.4349\u001B[0m\r\n",
      "Training 97     - 100    -          0.1565 -           0.9457    | 0.9461   - 7.1503\r\n",
      "\u001B[1;4mValidati 97     - 100    -          0.3983 -           0.8782    | 0.8781   - 0.4817\u001B[0m\r\n",
      "Training 98     - 100    -          0.1568 -           0.9457    | 0.9463   - 7.0577\r\n",
      "\u001B[1;4mValidati 98     - 100    -          0.4330 -           0.8676    | 0.8706   - 0.4827\u001B[0m\r\n",
      "Training 99     - 100    -          0.1583 -           0.9450    | 0.9455   - 7.1873\r\n",
      "\u001B[1;4mValidati 99     - 100    -          0.4536 -           0.8604    | 0.8621   - 0.4324\u001B[0m\r\n",
      "Training 100    - 100    -          0.1554 -           0.9465    | 0.9472   - 7.1642\r\n",
      "\u001B[1;4mValidati 100    - 100    -          0.4010 -           0.8762    | 0.8793   - 0.4357\u001B[0m\r\n",
      "Training 101    - 100    -          0.1619 -           0.9443    | 0.9444   - 7.2480\r\n",
      "\u001B[1;4mValidati 101    - 100    -          0.4048 -           0.8752    | 0.8786   - 0.4368\u001B[0m\r\n",
      "Training 102    - 100    -          0.1555 -           0.9452    | 0.9462   - 7.0343\r\n",
      "\u001B[1;4mValidati 102    - 100    -          0.6752 -           0.8134    | 0.8172   - 0.4424\u001B[0m\r\n",
      "Training 103    - 100    -          0.1536 -           0.9479    | 0.9478   - 7.1141\r\n",
      "\u001B[1;4mValidati 103    - 100    -          0.5425 -           0.8441    | 0.8480   - 0.4724\u001B[0m\r\n",
      "Training 104    - 100    -          0.1585 -           0.9438    | 0.9441   - 7.2411\r\n",
      "\u001B[1;4mValidati 104    - 100    -          0.4403 -           0.8704    | 0.8725   - 0.4528\u001B[0m\r\n",
      "Training 105    - 100    -          0.1544 -           0.9464    | 0.9469   - 7.0974\r\n",
      "\u001B[1;4mValidati 105    - 100    -          0.4822 -           0.8548    | 0.8577   - 0.4735\u001B[0m\r\n",
      "Training 106    - 100    -          0.1574 -           0.9452    | 0.9459   - 7.0915\r\n",
      "\u001B[1;4mValidati 106    - 100    -          0.4366 -           0.8635    | 0.8686   - 0.4607\u001B[0m\r\n",
      "Training 107    - 100    -          0.1522 -           0.9469    | 0.9471   - 6.9628\r\n",
      "\u001B[1;4mValidati 107    - 100    -          0.4300 -           0.8624    | 0.8642   - 0.4325\u001B[0m\r\n",
      "Training 108    - 100    -          0.1571 -           0.9462    | 0.9460   - 7.0053\r\n",
      "\u001B[1;4mValidati 108    - 100    -          0.4889 -           0.8523    | 0.8539   - 0.4289\u001B[0m\r\n",
      "Training 109    - 100    -          0.1635 -           0.9426    | 0.9425   - 6.9229\r\n",
      "\u001B[1;4mValidati 109    - 100    -          0.4523 -           0.8634    | 0.8652   - 0.4264\u001B[0m\r\n",
      "Training 110    - 100    -          0.1523 -           0.9475    | 0.9479   - 6.9208\r\n",
      "\u001B[1;4mValidati 110    - 100    -          0.4041 -           0.8803    | 0.8805   - 0.4427\u001B[0m\r\n",
      "Training 111    - 100    -          0.1450 -           0.9491    | 0.9492   - 6.9624\r\n",
      "\u001B[1;4mValidati 111    - 100    -          0.4662 -           0.8571    | 0.8589   - 0.4204\u001B[0m\r\n",
      "Training 112    - 100    -          0.1540 -           0.9460    | 0.9467   - 7.0525\r\n",
      "\u001B[1;4mValidati 112    - 100    -          0.6096 -           0.8257    | 0.8273   - 0.4554\u001B[0m\r\n",
      "Training 113    - 100    -          0.1526 -           0.9473    | 0.9473   - 7.0336\r\n",
      "\u001B[1;4mValidati 113    - 100    -          0.4217 -           0.8671    | 0.8714   - 0.4419\u001B[0m\r\n",
      "Training 114    - 100    -          0.1512 -           0.9467    | 0.9472   - 7.1041\r\n",
      "\u001B[1;4mValidati 114    - 100    -          0.4876 -           0.8562    | 0.8582   - 0.4526\u001B[0m\r\n",
      "Training 115    - 100    -          0.1475 -           0.9488    | 0.9495   - 7.1276\r\n",
      "\u001B[1;4mValidati 115    - 100    -          0.5037 -           0.8520    | 0.8557   - 0.4395\u001B[0m\r\n",
      "Training 116    - 100    -          0.1451 -           0.9502    | 0.9505   - 6.8832\r\n",
      "\u001B[1;4mValidati 116    - 100    -          0.4448 -           0.8690    | 0.8724   - 0.4345\u001B[0m\r\n",
      "Training 117    - 100    -          0.1521 -           0.9466    | 0.9472   - 7.1504\r\n",
      "\u001B[1;4mValidati 117    - 100    -          0.5256 -           0.8408    | 0.8422   - 0.4295\u001B[0m\r\n",
      "Training 118    - 100    -          0.1492 -           0.9487    | 0.9489   - 7.2266\r\n",
      "\u001B[1;4mValidati 118    - 100    -          0.5394 -           0.8453    | 0.8476   - 0.5249\u001B[0m\r\n",
      "Training 119    - 100    -          0.1485 -           0.9491    | 0.9491   - 7.6554\r\n",
      "\u001B[1;4mValidati 119    - 100    -          0.5143 -           0.8433    | 0.8463   - 0.4304\u001B[0m\r\n",
      "Training 120    - 100    -          0.1503 -           0.9475    | 0.9475   - 7.0965\r\n",
      "\u001B[1;4mValidati 120    - 100    -          0.3920 -           0.8783    | 0.8814   - 0.4334\u001B[0m\r\n",
      "Training 121    - 100    -          0.0699 -           0.9778    | 0.9775   - 6.8929\r\n",
      "\u001B[1;4mValidati 121    - 100    -          0.3022 -           0.9085    | 0.9095   - 0.4490\u001B[0m\r\n",
      "Training 122    - 100    -          0.0466 -           0.9861    | 0.9857   - 7.0406\r\n",
      "\u001B[1;4mValidati 122    - 100    -          0.2933 -           0.9156    | 0.9173   - 0.4403\u001B[0m\r\n",
      "Training 123    - 100    -          0.0368 -           0.9894    | 0.9892   - 7.1159\r\n",
      "\u001B[1;4mValidati 123    - 100    -          0.2993 -           0.9174    | 0.9196   - 0.4431\u001B[0m\r\n",
      "Training 124    - 100    -          0.0321 -           0.9908    | 0.9907   - 7.0553\r\n",
      "\u001B[1;4mValidati 124    - 100    -          0.3101 -           0.9134    | 0.9148   - 0.4644\u001B[0m\r\n",
      "Training 125    - 100    -          0.0287 -           0.9920    | 0.9920   - 7.3084\r\n",
      "\u001B[1;4mValidati 125    - 100    -          0.3184 -           0.9139    | 0.9155   - 0.4270\u001B[0m\r\n",
      "Training 126    - 100    -          0.0259 -           0.9927    | 0.9926   - 7.2503\r\n",
      "\u001B[1;4mValidati 126    - 100    -          0.3181 -           0.9133    | 0.9152   - 0.4368\u001B[0m\r\n",
      "Training 127    - 100    -          0.0223 -           0.9939    | 0.9939   - 7.0443\r\n",
      "\u001B[1;4mValidati 127    - 100    -          0.3242 -           0.9134    | 0.9140   - 0.4318\u001B[0m\r\n",
      "Training 128    - 100    -          0.0219 -           0.9940    | 0.9938   - 7.0831\r\n",
      "\u001B[1;4mValidati 128    - 100    -          0.3195 -           0.9142    | 0.9155   - 0.4578\u001B[0m\r\n",
      "Training 129    - 100    -          0.0220 -           0.9935    | 0.9936   - 7.3510\r\n",
      "\u001B[1;4mValidati 129    - 100    -          0.3345 -           0.9130    | 0.9146   - 0.4807\u001B[0m\r\n",
      "Training 130    - 100    -          0.0185 -           0.9953    | 0.9951   - 7.0639\r\n",
      "\u001B[1;4mValidati 130    - 100    -          0.3186 -           0.9181    | 0.9201   - 0.4830\u001B[0m\r\n",
      "Training 131    - 100    -          0.0176 -           0.9954    | 0.9953   - 7.0703\r\n",
      "\u001B[1;4mValidati 131    - 100    -          0.3196 -           0.9173    | 0.9186   - 0.4424\u001B[0m\r\n",
      "Training 132    - 100    -          0.0183 -           0.9950    | 0.9950   - 7.1276\r\n",
      "\u001B[1;4mValidati 132    - 100    -          0.3246 -           0.9180    | 0.9199   - 0.4626\u001B[0m\r\n",
      "Training 133    - 100    -          0.0160 -           0.9958    | 0.9957   - 7.1099\r\n",
      "\u001B[1;4mValidati 133    - 100    -          0.3236 -           0.9160    | 0.9172   - 0.4294\u001B[0m\r\n",
      "Training 134    - 100    -          0.0158 -           0.9960    | 0.9959   - 6.9936\r\n",
      "\u001B[1;4mValidati 134    - 100    -          0.3224 -           0.9169    | 0.9183   - 0.4592\u001B[0m\r\n",
      "Training 135    - 100    -          0.0146 -           0.9964    | 0.9963   - 7.2148\r\n",
      "\u001B[1;4mValidati 135    - 100    -          0.3361 -           0.9147    | 0.9154   - 0.4805\u001B[0m\r\n",
      "Training 136    - 100    -          0.0131 -           0.9967    | 0.9968   - 7.1973\r\n",
      "\u001B[1;4mValidati 136    - 100    -          0.3370 -           0.9151    | 0.9160   - 0.4388\u001B[0m\r\n",
      "Training 137    - 100    -          0.0136 -           0.9964    | 0.9964   - 7.2696\r\n",
      "\u001B[1;4mValidati 137    - 100    -          0.3308 -           0.9178    | 0.9194   - 0.4397\u001B[0m\r\n",
      "Training 138    - 100    -          0.0126 -           0.9969    | 0.9969   - 7.0730\r\n",
      "\u001B[1;4mValidati 138    - 100    -          0.3390 -           0.9152    | 0.9159   - 0.4440\u001B[0m\r\n",
      "Training 139    - 100    -          0.0126 -           0.9967    | 0.9967   - 6.9837\r\n",
      "\u001B[1;4mValidati 139    - 100    -          0.3372 -           0.9177    | 0.9181   - 0.4346\u001B[0m\r\n",
      "Training 140    - 100    -          0.0128 -           0.9965    | 0.9965   - 7.2406\r\n",
      "\u001B[1;4mValidati 140    - 100    -          0.3511 -           0.9147    | 0.9158   - 0.4898\u001B[0m\r\n",
      "Training 141    - 100    -          0.0129 -           0.9968    | 0.9967   - 7.2236\r\n",
      "\u001B[1;4mValidati 141    - 100    -          0.3349 -           0.9192    | 0.9205   - 0.4454\u001B[0m\r\n",
      "Training 142    - 100    -          0.0124 -           0.9970    | 0.9969   - 7.2744\r\n",
      "\u001B[1;4mValidati 142    - 100    -          0.3352 -           0.9172    | 0.9190   - 0.4409\u001B[0m\r\n",
      "Training 143    - 100    -          0.0133 -           0.9965    | 0.9965   - 7.0495\r\n",
      "\u001B[1;4mValidati 143    - 100    -          0.3433 -           0.9141    | 0.9151   - 0.4193\u001B[0m\r\n",
      "Training 144    - 100    -          0.0135 -           0.9965    | 0.9965   - 6.9995\r\n",
      "\u001B[1;4mValidati 144    - 100    -          0.3317 -           0.9165    | 0.9188   - 0.4295\u001B[0m\r\n",
      "Training 145    - 100    -          0.0116 -           0.9973    | 0.9973   - 6.9429\r\n",
      "\u001B[1;4mValidati 145    - 100    -          0.3526 -           0.9144    | 0.9154   - 0.4259\u001B[0m\r\n",
      "Training 146    - 100    -          0.0104 -           0.9976    | 0.9977   - 7.0192\r\n",
      "\u001B[1;4mValidati 146    - 100    -          0.3457 -           0.9161    | 0.9179   - 0.4388\u001B[0m\r\n",
      "Training 147    - 100    -          0.0122 -           0.9967    | 0.9968   - 6.9771\r\n",
      "\u001B[1;4mValidati 147    - 100    -          0.3461 -           0.9155    | 0.9173   - 0.4259\u001B[0m\r\n",
      "Training 148    - 100    -          0.0114 -           0.9970    | 0.9970   - 6.8915\r\n",
      "\u001B[1;4mValidati 148    - 100    -          0.3510 -           0.9156    | 0.9170   - 0.4279\u001B[0m\r\n",
      "Training 149    - 100    -          0.0107 -           0.9975    | 0.9975   - 6.9165\r\n",
      "\u001B[1;4mValidati 149    - 100    -          0.3441 -           0.9155    | 0.9171   - 0.4315\u001B[0m\r\n",
      "Training 150    - 100    -          0.0121 -           0.9967    | 0.9967   - 6.9220\r\n",
      "\u001B[1;4mValidati 150    - 100    -          0.3556 -           0.9126    | 0.9136   - 0.4340\u001B[0m\r\n",
      "Training 151    - 100    -          0.0138 -           0.9961    | 0.9960   - 6.8984\r\n",
      "\u001B[1;4mValidati 151    - 100    -          0.3465 -           0.9134    | 0.9151   - 0.4326\u001B[0m\r\n",
      "Training 152    - 100    -          0.0109 -           0.9973    | 0.9972   - 7.1403\r\n",
      "\u001B[1;4mValidati 152    - 100    -          0.3444 -           0.9171    | 0.9182   - 0.4562\u001B[0m\r\n",
      "Training 153    - 100    -          0.0112 -           0.9970    | 0.9970   - 7.2495\r\n",
      "\u001B[1;4mValidati 153    - 100    -          0.3513 -           0.9129    | 0.9143   - 0.4222\u001B[0m\r\n",
      "Training 154    - 100    -          0.0121 -           0.9969    | 0.9969   - 7.0150\r\n",
      "\u001B[1;4mValidati 154    - 100    -          0.3441 -           0.9150    | 0.9162   - 0.4303\u001B[0m\r\n",
      "Training 155    - 100    -          0.0118 -           0.9970    | 0.9969   - 6.9924\r\n",
      "\u001B[1;4mValidati 155    - 100    -          0.3523 -           0.9143    | 0.9157   - 0.4332\u001B[0m\r\n",
      "Training 156    - 100    -          0.0125 -           0.9965    | 0.9965   - 7.0711\r\n",
      "\u001B[1;4mValidati 156    - 100    -          0.3717 -           0.9115    | 0.9128   - 0.4315\u001B[0m\r\n",
      "Training 157    - 100    -          0.0124 -           0.9968    | 0.9968   - 6.9106\r\n",
      "\u001B[1;4mValidati 157    - 100    -          0.3811 -           0.9108    | 0.9123   - 0.4346\u001B[0m\r\n",
      "Training 158    - 100    -          0.0123 -           0.9969    | 0.9968   - 6.9007\r\n",
      "\u001B[1;4mValidati 158    - 100    -          0.3666 -           0.9119    | 0.9132   - 0.4394\u001B[0m\r\n",
      "Training 159    - 100    -          0.0137 -           0.9960    | 0.9959   - 6.8662\r\n",
      "\u001B[1;4mValidati 159    - 100    -          0.3635 -           0.9107    | 0.9123   - 0.4162\u001B[0m\r\n",
      "Training 160    - 100    -          0.0153 -           0.9953    | 0.9953   - 7.0131\r\n",
      "\u001B[1;4mValidati 160    - 100    -          0.3780 -           0.9077    | 0.9091   - 0.4421\u001B[0m\r\n",
      "Training 161    - 100    -          0.0102 -           0.9974    | 0.9975   - 6.9981\r\n",
      "\u001B[1;4mValidati 161    - 100    -          0.3528 -           0.9155    | 0.9164   - 0.4358\u001B[0m\r\n",
      "Training 162    - 100    -          0.0078 -           0.9984    | 0.9983   - 7.0955\r\n",
      "\u001B[1;4mValidati 162    - 100    -          0.3460 -           0.9176    | 0.9191   - 0.4193\u001B[0m\r\n",
      "Training 163    - 100    -          0.0064 -           0.9988    | 0.9987   - 6.9813\r\n",
      "\u001B[1;4mValidati 163    - 100    -          0.3460 -           0.9196    | 0.9202   - 0.4553\u001B[0m\r\n",
      "Training 164    - 100    -          0.0056 -           0.9992    | 0.9991   - 7.0014\r\n",
      "\u001B[1;4mValidati 164    - 100    -          0.3434 -           0.9191    | 0.9206   - 0.4463\u001B[0m\r\n",
      "Training 165    - 100    -          0.0056 -           0.9989    | 0.9990   - 6.9168\r\n",
      "\u001B[1;4mValidati 165    - 100    -          0.3379 -           0.9210    | 0.9217   - 0.4141\u001B[0m\r\n",
      "Training 166    - 100    -          0.0054 -           0.9989    | 0.9989   - 6.8355\r\n",
      "\u001B[1;4mValidati 166    - 100    -          0.3408 -           0.9205    | 0.9214   - 0.4377\u001B[0m\r\n",
      "Training 167    - 100    -          0.0052 -           0.9991    | 0.9990   - 7.1004\r\n",
      "\u001B[1;4mValidati 167    - 100    -          0.3378 -           0.9205    | 0.9213   - 0.4345\u001B[0m\r\n",
      "Training 168    - 100    -          0.0052 -           0.9991    | 0.9991   - 7.1124\r\n",
      "\u001B[1;4mValidati 168    - 100    -          0.3454 -           0.9192    | 0.9204   - 0.4377\u001B[0m\r\n",
      "Training 169    - 100    -          0.0050 -           0.9992    | 0.9992   - 6.9839\r\n",
      "\u001B[1;4mValidati 169    - 100    -          0.3442 -           0.9199    | 0.9207   - 0.4628\u001B[0m\r\n",
      "Training 170    - 100    -          0.0048 -           0.9992    | 0.9992   - 6.9775\r\n",
      "\u001B[1;4mValidati 170    - 100    -          0.3392 -           0.9203    | 0.9213   - 0.4308\u001B[0m\r\n",
      "Training 171    - 100    -          0.0044 -           0.9994    | 0.9993   - 7.1397\r\n",
      "\u001B[1;4mValidati 171    - 100    -          0.3373 -           0.9202    | 0.9207   - 0.4470\u001B[0m\r\n",
      "Training 172    - 100    -          0.0043 -           0.9993    | 0.9993   - 6.8661\r\n",
      "\u001B[1;4mValidati 172    - 100    -          0.3295 -           0.9209    | 0.9219   - 0.4293\u001B[0m\r\n",
      "Training 173    - 100    -          0.0045 -           0.9993    | 0.9993   - 7.0467\r\n",
      "\u001B[1;4mValidati 173    - 100    -          0.3337 -           0.9222    | 0.9235   - 0.4124\u001B[0m\r\n",
      "Training 174    - 100    -          0.0043 -           0.9994    | 0.9994   - 7.0756\r\n",
      "\u001B[1;4mValidati 174    - 100    -          0.3412 -           0.9200    | 0.9211   - 0.4414\u001B[0m\r\n",
      "Training 175    - 100    -          0.0042 -           0.9995    | 0.9994   - 6.9968\r\n",
      "\u001B[1;4mValidati 175    - 100    -          0.3423 -           0.9204    | 0.9209   - 0.4298\u001B[0m\r\n",
      "Training 176    - 100    -          0.0038 -           0.9996    | 0.9996   - 6.9621\r\n",
      "\u001B[1;4mValidati 176    - 100    -          0.3423 -           0.9209    | 0.9220   - 0.4374\u001B[0m\r\n",
      "Training 177    - 100    -          0.0037 -           0.9997    | 0.9997   - 7.0792\r\n",
      "\u001B[1;4mValidati 177    - 100    -          0.3343 -           0.9211    | 0.9224   - 0.4630\u001B[0m\r\n",
      "Training 178    - 100    -          0.0041 -           0.9994    | 0.9994   - 7.0563\r\n",
      "\u001B[1;4mValidati 178    - 100    -          0.3357 -           0.9198    | 0.9218   - 0.4400\u001B[0m\r\n",
      "Training 179    - 100    -          0.0043 -           0.9994    | 0.9994   - 6.8998\r\n",
      "\u001B[1;4mValidati 179    - 100    -          0.3300 -           0.9208    | 0.9217   - 0.4204\u001B[0m\r\n",
      "Training 180    - 100    -          0.0040 -           0.9994    | 0.9994   - 7.1743\r\n",
      "\u001B[1;4mValidati 180    - 100    -          0.3339 -           0.9203    | 0.9211   - 0.4364\u001B[0m\r\n",
      "Training 181    - 100    -          0.0037 -           0.9996    | 0.9995   - 6.8423\r\n",
      "\u001B[1;4mValidati 181    - 100    -          0.3307 -           0.9216    | 0.9224   - 0.4261\u001B[0m\r\n",
      "Training 182    - 100    -          0.0041 -           0.9993    | 0.9993   - 7.0078\r\n",
      "\u001B[1;4mValidati 182    - 100    -          0.3397 -           0.9199    | 0.9215   - 0.4174\u001B[0m\r\n",
      "Training 183    - 100    -          0.0037 -           0.9995    | 0.9995   - 6.8397\r\n",
      "\u001B[1;4mValidati 183    - 100    -          0.3336 -           0.9211    | 0.9227   - 0.4143\u001B[0m\r\n",
      "Training 184    - 100    -          0.0037 -           0.9996    | 0.9996   - 6.9919\r\n",
      "\u001B[1;4mValidati 184    - 100    -          0.3308 -           0.9207    | 0.9218   - 0.4282\u001B[0m\r\n",
      "Training 185    - 100    -          0.0033 -           0.9997    | 0.9997   - 7.1426\r\n",
      "\u001B[1;4mValidati 185    - 100    -          0.3345 -           0.9201    | 0.9211   - 0.4551\u001B[0m\r\n",
      "Training 186    - 100    -          0.0036 -           0.9996    | 0.9996   - 7.1766\r\n",
      "\u001B[1;4mValidati 186    - 100    -          0.3409 -           0.9183    | 0.9197   - 0.4227\u001B[0m\r\n",
      "Training 187    - 100    -          0.0037 -           0.9995    | 0.9995   - 6.9429\r\n",
      "\u001B[1;4mValidati 187    - 100    -          0.3334 -           0.9207    | 0.9214   - 0.4519\u001B[0m\r\n",
      "Training 188    - 100    -          0.0037 -           0.9995    | 0.9995   - 6.9940\r\n",
      "\u001B[1;4mValidati 188    - 100    -          0.3308 -           0.9226    | 0.9229   - 0.4270\u001B[0m\r\n",
      "Training 189    - 100    -          0.0036 -           0.9994    | 0.9994   - 7.0334\r\n",
      "\u001B[1;4mValidati 189    - 100    -          0.3289 -           0.9234    | 0.9233   - 0.4263\u001B[0m\r\n",
      "Training 190    - 100    -          0.0034 -           0.9995    | 0.9995   - 7.1402\r\n",
      "\u001B[1;4mValidati 190    - 100    -          0.3374 -           0.9205    | 0.9205   - 0.4341\u001B[0m\r\n",
      "Training 191    - 100    -          0.0036 -           0.9995    | 0.9995   - 6.9957\r\n",
      "\u001B[1;4mValidati 191    - 100    -          0.3415 -           0.9205    | 0.9220   - 0.4300\u001B[0m\r\n",
      "Training 192    - 100    -          0.0033 -           0.9996    | 0.9996   - 6.8947\r\n",
      "\u001B[1;4mValidati 192    - 100    -          0.3392 -           0.9189    | 0.9199   - 0.4320\u001B[0m\r\n",
      "Training 193    - 100    -          0.0031 -           0.9997    | 0.9997   - 6.9692\r\n",
      "\u001B[1;4mValidati 193    - 100    -          0.3339 -           0.9211    | 0.9215   - 0.4294\u001B[0m\r\n",
      "Training 194    - 100    -          0.0031 -           0.9997    | 0.9997   - 6.9104\r\n",
      "\u001B[1;4mValidati 194    - 100    -          0.3362 -           0.9206    | 0.9216   - 0.4127\u001B[0m\r\n",
      "Training 195    - 100    -          0.0031 -           0.9998    | 0.9998   - 6.9674\r\n",
      "\u001B[1;4mValidati 195    - 100    -          0.3386 -           0.9215    | 0.9229   - 0.4145\u001B[0m\r\n",
      "Training 196    - 100    -          0.0032 -           0.9996    | 0.9996   - 6.9379\r\n",
      "\u001B[1;4mValidati 196    - 100    -          0.3455 -           0.9201    | 0.9204   - 0.4255\u001B[0m\r\n",
      "Training 197    - 100    -          0.0030 -           0.9997    | 0.9996   - 7.0707\r\n",
      "\u001B[1;4mValidati 197    - 100    -          0.3321 -           0.9230    | 0.9230   - 0.4277\u001B[0m\r\n",
      "Training 198    - 100    -          0.0033 -           0.9995    | 0.9995   - 6.9866\r\n",
      "\u001B[1;4mValidati 198    - 100    -          0.3285 -           0.9224    | 0.9231   - 0.4323\u001B[0m\r\n",
      "Training 199    - 100    -          0.0029 -           0.9997    | 0.9997   - 6.9042\r\n",
      "\u001B[1;4mValidati 199    - 100    -          0.3273 -           0.9222    | 0.9230   - 0.4512\u001B[0m\r\n",
      "Training 200    - 100    -          0.0030 -           0.9997    | 0.9997   - 6.9886\r\n",
      "\u001B[1;4mValidati 200    - 100    -          0.3274 -           0.9222    | 0.9232   - 0.4253\u001B[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = 0\n",
    "end_epoch = 200\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    \n",
    "    tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SummaryWriter' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-8e27fa4579dc>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfigsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m30\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m14\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msubplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mpp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"val/acc\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mpp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"val/f1\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlegend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-27-8e27fa4579dc>\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(k)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m200\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0msm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvolve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mones\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'same'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mpp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtensorboard\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34mf\"{k} = {max(tensorboard.history[k])}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mspp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensorboard\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34mf\"{k} = {max(tensorboard.history[k])}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'SummaryWriter' object has no attribute 'history'"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1008 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(range(200))\n",
    "sm = lambda y, w: np.convolve(y, np.ones(w)/w, mode='same')\n",
    "pp = lambda k: plt.plot(x, tensorboard.history[k], label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "spp = lambda k: plt.plot(x, sm(tensorboard.history[k], 5), label=f\"{k} = {max(tensorboard.history[k])}\")\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(30, 14))\n",
    "plt.subplot(2, 3, 1)\n",
    "pp(\"val/acc\")\n",
    "pp(\"val/f1\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "pp(\"detail_hyperparameters/learning_rate\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .llll||=||llll."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}